---
layout: post
title: "SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models"
description:
date: 2025-03-06 16:25:10 +09:00
tags: AI
categories: Paper
giscus_comments: true
related_posts: false

featured: false
pretty_table: true

toc:
  beginning: false  # ë§¨ ì•ì— ëª©ì°¨
  sidebar: left  # ëª©ì°¨ê°€ ì‚¬ì´ë“œë°” ì™¼ìª½ì— ë¶™ì–´ìˆìŒ
---

Authors: Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han
MIT, NVIDIA, CMU, Princeton, UC Berkeley, SJTU, Pika Labs
Venue & Year: 25, ICLR, Spotlight
ë‚ ì§œ: 2025ë…„ 3ì›” 6ì¼

| ArXiv | [https://arxiv.org/abs/2411.05007](https://arxiv.org/abs/2411.05007) |
| --- | --- |
| Project Page | [https://hanlab.mit.edu/projects/svdquant](https://hanlab.mit.edu/projects/svdquant) |
| Github Code | [https://github.com/mit-han-lab/nunchaku](https://github.com/mit-han-lab/nunchaku) |
| Demo | [https://svdquant.mit.edu/](https://svdquant.mit.edu/) |

> ğŸ’¡
> 
> **Key Differentiator**
> 
> â€œOutlier Absorption Using Singular Value Decompositionâ€
> 
> 
{: .block-warning }

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

# Song Han?

> Song Han is an associate professor at MIT EECS. He earned his PhD from Stanford, pioneering efficient AI computing techniques such as â€œDeep Compressionâ€ (pruning, quantization) and the â€œEfficient Inference Engine,â€ which first introduced weight sparsity to modern AI chips, making it one of the top-5 most cited papers in the 50-year history of ISCA (1953-2023). His innovations, including TinyML and hardware-aware neural architecture search (Once-for-All Network), have advanced AI model deployment on resource-constrained devices.
> 

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%201.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

# 1. Introduction

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%202.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

LLMê³¼ ë¹„êµí–ˆì„ ë•Œ, ëª¨ë¸ ì‚¬ì´ì¦ˆì— ë”°ë¼ ê³„ì‚° ë¹„ìš©ì´ ë¹ ë¥´ê²Œ ì¦ê°€í•œë‹¤.

Mooreâ€™s lawê°€ slow down í•¨ìœ¼ë¡œì„œ, ì €ë ´í•œ ì¶”ë¡ (low-precision inference) ìœ¼ë¡œ ì „í™˜í•˜ëŠ”ì¤‘ 

â†’ 4bit floating point (FP4)ê°€ ëŒ€ì„¸ì„

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%203.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

LLM

latencyëŠ” ì£¼ë¡œ ê°€ì¤‘ì¹˜(weight) ë¡œë”© ì†ë„ì— ì˜í•´ ê²°ì •

"ê°€ì¤‘ì¹˜ë§Œ ì–‘ìí™”(weight-only quantization)" í•´ë„ ì†ë„ë¥¼ ê°œì„ 

Diffusion ëª¨ë¸

ë ˆì´í„´ì‹œëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì†ë„ê°€ ì•„ë‹ˆë¼, ì—°ì‚°ëŸ‰ ìì²´ê°€ ë³‘ëª©

ì™œëƒí•˜ë©´ ê°€ì¤‘ì¹˜ë§Œ 4ë¹„íŠ¸ë¡œ ì¤„ì—¬ë„ í™œì„±í™”ê°’ì´ 16ë¹„íŠ¸ì´ë©´, ì—°ì‚° ê³¼ì •ì—ì„œ 16ë¹„íŠ¸ë¡œ ë‹¤ì‹œ ë³€í™˜(upcast)ë˜ë¯€ë¡œ ì—°ì‚°ëŸ‰ì´ ì¤„ì–´ë“¤ì§€ ì•ŠìŒ.

ê²°êµ­ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ë ¤ë©´ ê°€ì¤‘ì¹˜(weight)ë¿ë§Œ ì•„ë‹ˆë¼ **í™œì„±í™”ê°’(activation)**ë„ í•¨ê»˜ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”í•´ì•¼ í•¨.

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%204.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

> ğŸ“¢
> 
> - **Input Channel** â†’ ì›ë˜ Activationì—ì„œ ë‚˜ì˜¨ ì…ë ¥ ì±„ë„
> - **Channel** â†’ Weightì˜ ê° ì±„ë„
{: .block-warning }

### 1. ê¸°ì¡´ 4ë¹„íŠ¸ ì–‘ìí™”(4-bit Quantization)ì˜ ë¬¸ì œì 

- ê°€ì¤‘ì¹˜(Weight)ì™€ í™œì„±í™”ê°’(Activation) ëª¨ë‘ 4ë¹„íŠ¸ë¡œ ì¤„ì´ë©´ í’ˆì§ˆì´ í¬ê²Œ ì €í•˜ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ.
- íŠ¹íˆ ê¸°ì¡´ ë°©ë²•(ì˜ˆ: Smoothing)ì€ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™”ê°’ ì‚¬ì´ì—ì„œ Outlierë¥¼ ì´ë™ì‹œí‚¤ëŠ” ë°©ì‹ì„ ì‚¬ìš©í–ˆì§€ë§Œ,Diffusion ëª¨ë¸ì—ì„œëŠ” Outlierê°€ ì–‘ìª½(W, X) ëª¨ë‘ì—ì„œ ì‹¬ê°í•˜ê²Œ ë°œìƒí•˜ë¯€ë¡œ íš¨ê³¼ì ì´ì§€ ì•ŠìŒ.
    - ê¸°ì¡´ ë°©ì‹ì€ í™œì„±í™”ê°’(X)ì—ì„œ Outlierë¥¼ ì œê±°í•˜ë ¤ê³  í•˜ë©´ ê°€ì¤‘ì¹˜(W)ë¡œ ì´ë™í•˜ê³ , ë°˜ëŒ€ë¡œ í•˜ë©´ Xì— Outlierê°€ ë‚¨ëŠ” ë¬¸ì œ ë°œìƒ.



### 2. SVDQuantì˜ í•µì‹¬ ì•„ì´ë””ì–´

âœ… Outlierë¥¼ ë‹¨ìˆœíˆ ì´ë™í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, "**í¡ìˆ˜**"í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•¨.

âœ… ì €ë¹„ìš©ì˜ "Low-Rank Branch"ë¥¼ ì¶”ê°€í•˜ì—¬ Outlierë¥¼ ê°€ì¤‘ì¹˜(W)ì—ì„œ í¡ìˆ˜í•¨.

âœ… ì´ë¥¼ ìœ„í•´ SVD(Singular Value Decomposition, íŠ¹ì´ê°’ ë¶„í•´) ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ë‘ ê°œì˜ ì„±ë¶„ìœ¼ë¡œ ë¶„í•´í•¨.



### 3. SVDQuantì˜ ë‹¨ê³„ë³„ ë™ì‘ ë°©ì‹

1ï¸âƒ£ Outlier ì´ë™ (Smoothing)

- ë¨¼ì € Outlierë¥¼ í™œì„±í™”ê°’(X)ì—ì„œ ê°€ì¤‘ì¹˜(W)ë¡œ ì´ë™í•¨.
- ì´ë¥¼ í†µí•´ í™œì„±í™”ê°’(X)ì´ ë” ê· ì¼í•´ì ¸ì„œ 4ë¹„íŠ¸ ì–‘ìí™”ê°€ ë” ì‰¬ì›Œì§.

2ï¸âƒ£ SVD(íŠ¹ì´ê°’ ë¶„í•´)ë¥¼ ì ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜(W)ë¥¼ ë‘ ê°œì˜ ì„±ë¶„ìœ¼ë¡œ ë¶„í•´

- W â†’ L1L2(ì €ìˆœìœ„ ì„±ë¶„) + ì”ì—¬ ì„±ë¶„(W - L1L2)ë¡œ ë¶„ë¦¬
- L1L2(ì €ìˆœìœ„ ì„±ë¶„)ì€ 16ë¹„íŠ¸ë¡œ ìœ ì§€í•˜ê³ , W - L1L2(ì”ì—¬ ì„±ë¶„)ë§Œ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”
- ì¦‰, ì €ìˆœìœ„ ì„±ë¶„(Low-Rank Component)ì´ Outlierë¥¼ í¡ìˆ˜í•˜ë©´ì„œ 4ë¹„íŠ¸ ì–‘ìí™”ê°€ ë” ì‰¬ì›Œì§.

3ï¸âƒ£ ì €ìˆœìœ„ ì„±ë¶„ì„ ë”°ë¡œ ê³„ì‚°í•˜ë©´ ë©”ëª¨ë¦¬ ì•¡ì„¸ìŠ¤ ì˜¤ë²„í—¤ë“œê°€ ì¦ê°€í•˜ëŠ” ë¬¸ì œ ë°œìƒ

- ì¦‰, L1L2ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬í•˜ë©´ ì—°ì‚° ì†ë„ê°€ ëŠë ¤ì§€ëŠ” ë¬¸ì œê°€ ìƒê¹€.
- ê¸°ë³¸ì ìœ¼ë¡œ 4ë¹„íŠ¸ ì—°ì‚°ì˜ ì†ë„ë¥¼ ë†’ì´ë ¤ê³  í–ˆëŠ”ë°, ì €ìˆœìœ„ ì—°ì‚°ì´ ì¶”ê°€ë˜ë©´ ì˜¤íˆë ¤ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ.

4ï¸âƒ£ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì „ìš© ì¶”ë¡  ì—”ì§„(Nunchaku) ì„¤ê³„

- Nunchaku ì—”ì§„ì€ 4ë¹„íŠ¸ ì–‘ìí™” ì—°ì‚°ê³¼ ì €ìˆœìœ„ ì—°ì‚°ì„ í•¨ê»˜ ìµœì í™”í•˜ì—¬ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì„.
- ì¦‰, L1L2(ì €ìˆœìœ„ ì—°ì‚°)ì™€ 4ë¹„íŠ¸ ì—°ì‚°ì„ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” ì»¤ë„(fusion kernel)ë¡œ ë³€í™˜í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”.
- ì´ë¥¼ í†µí•´ ì¶”ê°€ì ì¸ ì—°ì‚°ëŸ‰ì´ ìƒê¸°ë”ë¼ë„ ì‹¤ì œë¡œëŠ” 4ë¹„íŠ¸ ì—°ì‚°ì˜ ì†ë„ë¥¼ í–¥ìƒí•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨.

### **ê¸°ì¡´ ë°©ì‹(SmoothQuant, AWQ)ê³¼ SVDQuantì˜ ì°¨ì´**

| ë°©ë²• | ë°©ì‹ | Outlier ì²˜ë¦¬ ë°©ì‹ | ì ìš© ëŒ€ìƒ | ë¬¸ì œì  |
| --- | --- | --- | --- | --- |
| **SmoothQuant (2023)** | W4A4 | Input Channel(Activation) â†’ Channel(Weight) | LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸) | Outlierê°€ ê°€ì¤‘ì¹˜ì— ëˆ„ì ë¨ |
| **AWQ (2024)** | W4A4 | **ê°€ì¤‘ì¹˜ ì¤‘ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ë³´ì¡´í•˜ì—¬ ì–‘ìí™”** | LLM | Diffusion ëª¨ë¸ì—ì„œëŠ” í•œê³„ ê°€ëŠ¥ì„± |
| **SVDQuant (2024)** | W4A4 | **ì €ìˆœìœ„(Low-Rank) ì„±ë¶„ìœ¼ë¡œ Outlier í¡ìˆ˜** | Diffusion ëª¨ë¸ ìµœì í™” | ì¶”ê°€ ì—°ì‚°ì„ í•´ê²°í•´ì•¼ í•¨ |

### **1. SmoothQuant (2023) â€“ Activationì—ì„œ Weightë¡œ ì´ìƒì¹˜ ì´ë™**

SmoothQuantì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **í™œì„±í™”ê°’(Activation)ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìƒì¹˜ë¥¼ ê°€ì¤‘ì¹˜(Weight)ë¡œ ì´ë™**ì‹œí‚¤ëŠ” ê±°ì•¼â€‹Li á„ƒá…³á†¼ - 2024 - SVDQuanâ€¦.

- **ê¸°ì¡´ ë¬¸ì œ**
    - Transformer ê¸°ë°˜ ëª¨ë¸ì—ì„œ Self-Attention ì—°ì‚°ì´ ë§ì•„ì„œ **í™œì„±í™”ê°’(Activation)ì˜ ë²”ìœ„ê°€ ë„“ì–´ì§€ê³  ì´ìƒì¹˜ê°€ ë°œìƒ**í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„.
    - ì´ë¥¼ 8-bitì´ë‚˜ 4-bitë¡œ ì–‘ìí™”í•˜ë©´, ì‘ì€ ê°’ë“¤ì€ ëª¨ë‘ 0ì´ ë˜ê³ , ì •ë³´ ì†ì‹¤ì´ ì‹¬í•´ì§.
- **í•´ê²° ë°©ë²•**
    - í™œì„±í™”ê°’(Activation)ì˜ ì±„ë„ë³„ ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•˜ì—¬, **ì´ìƒì¹˜ë¥¼ ê°€ì¤‘ì¹˜(Weight) ìª½ìœ¼ë¡œ ì´ë™**ì‹œí‚´.
    - ì¦‰, ì›ë˜ Activation ê°’ì´ í¬ë©´, í•´ë‹¹ ì±„ë„ì„ ìŠ¤ì¼€ì¼ë§í•´ì„œ ì¤„ì´ê³ , ëŒ€ì‹  ê·¸ ê°’ì„ Weightì—ì„œ ë³´ìƒí•´ì£¼ëŠ” ë°©ì‹.
    - ì´ë ‡ê²Œ í•˜ë©´, **Activation ê°’ì´ ì–‘ìí™”í•  ë•Œ ì†ì‹¤ ì—†ì´ ë” ê· ë“±í•˜ê²Œ ë¶„í¬í•  ìˆ˜ ìˆìŒ**.
- **í•œê³„**
    - Weight ìª½ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ëª°ì•„ë„£ìœ¼ë©´, **Weightì˜ ê°’ì´ ì»¤ì§€ê³ , Weight ì–‘ìí™” ì‹œ ì˜¤ë¥˜ê°€ ì»¤ì§ˆ ê°€ëŠ¥ì„±**ì´ ìˆìŒ.
    - ë”°ë¼ì„œ **Weightë¥¼ 4-bitë¡œ ì–‘ìí™”í•  ê²½ìš° ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ**.



### **2. AWQ (Activation-aware Weight Quantization, 2024) â€“ Weightì—ì„œ Activationìœ¼ë¡œ ì´ìƒì¹˜ ì´ë™**

AWQëŠ” **Weightì˜ ì´ìƒì¹˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ Activationìœ¼ë¡œ ë¶„ì‚°ì‹œí‚¤ëŠ” ë°©ì‹**

- **ê¸°ì¡´ ë¬¸ì œ**
    - SmoothQuant ë°©ì‹ì²˜ëŸ¼ ì´ìƒì¹˜ë¥¼ Weight ìª½ìœ¼ë¡œ ì´ë™ì‹œí‚¤ë©´, Weightì˜ í¬ê¸°ê°€ ì»¤ì ¸ì„œ **Weightë¥¼ 4-bitë¡œ ì–‘ìí™”í•  ë•Œ ì •ë³´ ì†ì‹¤ì´ ë°œìƒ**í•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§.
    - íŠ¹íˆ, Weightì— ì´ìƒì¹˜ê°€ ë§ìœ¼ë©´, **ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•´ë„ ì–‘ìí™” ì˜¤ë¥˜ê°€ ì»¤ì§€ê³  ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œ**ê°€ ë°œìƒ.
- **í•´ê²° ë°©ë²•**
    - ëŒ€ì‹  **Weightì—ì„œ Activationìœ¼ë¡œ ì¼ë¶€ ì´ìƒì¹˜ë¥¼ ì´ë™ì‹œì¼œì„œ, Weightê°€ ì–‘ìí™”ë  ë•Œ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”**í•¨.
    - ì¦‰, ì¤‘ìš”í•œ Weight ê°’ì„ ë”°ë¡œ ë³´í˜¸í•˜ê³ , ë¶ˆí•„ìš”í•œ í° ê°’ì„ Activation ìª½ìœ¼ë¡œ ì´ë™ì‹œì¼œì„œ Weightë¥¼ ë” ê· ë“±í•œ ë¶„í¬ë¡œ ë§Œë“¤ë„ë¡ ì„¤ê³„.
- **í•œê³„**
    - Activationì˜ ë¶„í¬ê°€ ë‹¤ì‹œ ë„“ì–´ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆìŒ â†’ **Activationì„ ë‹¤ì‹œ 4-bitë¡œ ì–‘ìí™”í•  ê²½ìš° ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ë„ ìˆìŒ**.



### **3. SVDQuant (2024) â€“ Outlierë¥¼ Low-Rank Componentë¡œ ì´ë™**

SVDQuantëŠ” SmoothQuantì™€ AWQì˜ ë¬¸ì œì ì„ ëª¨ë‘ í•´ê²°í•˜ë ¤ê³ , **ì´ìƒì¹˜ë¥¼ ì´ë™ì‹œí‚¤ëŠ” ê²ƒë¿ë§Œ ì•„ë‹ˆë¼ Low-Rank Componentë¡œ í¡ìˆ˜**í•˜ëŠ” ë°©ì‹ì´ì•¼â€‹Li á„ƒá…³á†¼ - 2024 - SVDQuanâ€¦.

- **í•µì‹¬ ì•„ì´ë””ì–´**
    - SmoothQuantì²˜ëŸ¼ **Activationì˜ ì´ìƒì¹˜ë¥¼ Weightë¡œ ì´ë™**í•˜ë©´ì„œë„,
    - AWQì²˜ëŸ¼ **Weightì—ì„œ ë‹¤ì‹œ Activationìœ¼ë¡œ ì´ë™í•˜ëŠ” ëŒ€ì‹ , Low-Rank Componentë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥**.
    - ì¦‰, **ì´ìƒì¹˜ë¥¼ ì–‘ìí™”í•˜ì§€ ì•Šê³ , 16-bit Low-Rank Componentë¡œ ìœ ì§€**í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”.
- **ì¥ì **
    - SmoothQuantë‚˜ AWQì²˜ëŸ¼ **í•œìª½ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ëª°ì•„ë„£ì§€ ì•Šê³ , Low-Rank Branchê°€ ì´ìƒì¹˜ë¥¼ í¡ìˆ˜í•´ì„œ ì†ì‹¤ì„ ë§‰ìŒ**.
    - **Weightì™€ Activation ëª¨ë‘ ê· ë“±í•œ ë¶„í¬ë¥¼ ê°€ì§€ê²Œ ë˜ì–´, ì–‘ìí™” ì˜¤ë¥˜ê°€ ì¤„ì–´ë“¦**.
    - ì‹¤ì œ ì‹¤í—˜ì—ì„œë„ **SmoothQuant, AWQë³´ë‹¤ 4-bit ì–‘ìí™”ì—ì„œ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨**.



### **ê²°ë¡ **

- **SmoothQuant** â†’ **Activationì˜ ì´ìƒì¹˜ë¥¼ Weightë¡œ ì´ë™** (Weightì˜ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥ì„± ìˆìŒ)
- **AWQ** â†’ **Weightì˜ ì´ìƒì¹˜ë¥¼ Activationìœ¼ë¡œ ì´ë™** (Activationì˜ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥ì„± ìˆìŒ)
- **SVDQuant** â†’ **Weightì™€ Activationì—ì„œ Low-Rank Componentë¡œ ì´ë™** (ì´ìƒì¹˜ ìì²´ë¥¼ ì œê±°í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”)

ì¦‰, **SmoothQuantê³¼ AWQëŠ” ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ Outlierë¥¼ ë°œìƒí•˜ì§€ ì•Šë„ë¡ í•˜ë ¤ê³  í–ˆë˜ ì ‘ê·¼ë²•**, ë°˜ë©´ **SVDQuantëŠ” Outlier ìì²´ë¥¼ Low-Rankë¡œ ë¹¼ë²„ë¦¬ëŠ” ë°©ì‹ì´ë¼ ì •ë³´ ì†ì‹¤ì´ ê°€ì¥ ì ìŒ**.

# 3 QUANTIZATION PRELIMINARY

- **ì–‘ìí™”(Quantization)ì˜ ê¸°ë³¸ ê°œë…**
    - ë”¥ëŸ¬ë‹ì—ì„œ **ì–‘ìí™”ëŠ” ì—°ì‚° ì†ë„ë¥¼ ë†’ì´ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë°©ë²•**.
    - í…ì„œ Xë¥¼ ì–‘ìí™”í•˜ëŠ” ê³¼ì •:
        
        {% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%205.webp" class="img-fluid rounded z-depth-1" zoomable=true %}
        
        - ì—¬ê¸°ì„œ QXâ€‹ëŠ” **ì–‘ìí™”ëœ(low-bit) ê°’**.
        - sXëŠ” **ìŠ¤ì¼€ì¼ë§ íŒ©í„°(Scaling Factor)**.
        - qmaxëŠ” **ìµœëŒ€ ì–‘ìí™” ê°’**(ë¹„íŠ¸ ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§).
        - **4ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì–‘ìí™”(4-bit FP)ì—ì„œëŠ” qmax=6ì„.**
- **ì–‘ìí™”ëœ í–‰ë ¬ ì—°ì‚°**
    - ì„ í˜• ê³„ì¸µ(Linear Layer)ì—ì„œ ì…ë ¥ Xì™€ ê°€ì¤‘ì¹˜ Wê°€ ìˆì„ ë•Œ, ì—°ì‚°ì„ ì–‘ìí™”ëœ ê°’ìœ¼ë¡œ ê·¼ì‚¬:
    
    {% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%206.webp" class="img-fluid rounded z-depth-1" zoomable=true %}
    
    - ì¦‰, ì–‘ìí™”ëœ í…ì„œë¼ë¦¬ ì—°ì‚°í•œ í›„, ìŠ¤ì¼€ì¼ë§ íŒ©í„° sXâ€‹sWâ€‹ë¥¼ ê³±í•˜ì—¬ ë‹¤ì‹œ ì›ë˜ ê°’ì— ê°€ê¹ê²Œ ë³µì›í•¨.
- **GPUì—ì„œ ê°™ì€ ë¹„íŠ¸í­(bit width)ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ì´ìœ **
    - **ìµœì‹  GPUì—ì„œëŠ” ì…ë ¥(QX)ê³¼ ê°€ì¤‘ì¹˜(QW)ì˜ ë¹„íŠ¸ ìˆ˜ê°€ ë™ì¼í•´ì•¼ ì—°ì‚° ì†ë„ê°€ í–¥ìƒë¨.**
    - **ë§Œì•½ QXì™€ QWì˜ ë¹„íŠ¸ ìˆ˜ê°€ ë‹¤ë¥´ë©´, ë” ë†’ì€ ë¹„íŠ¸ ê°’ìœ¼ë¡œ ë³€í™˜(upcast)ë˜ë©´ì„œ ì†ë„ ì´ì ì´ ì‚¬ë¼ì§.**
    - ì˜ˆ:
        - **ê°€ì¤‘ì¹˜(W)ë¥¼ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”(W4)í–ˆì§€ë§Œ, í™œì„±í™”ê°’(X)ì´ 16ë¹„íŠ¸(A16)ë¼ë©´?**
        â†’ **ì—°ì‚° ì‹œ W4ê°€ A16ìœ¼ë¡œ ì—…ìºìŠ¤íŠ¸(Upcast)ë˜ì–´ ì‹¤ì œ ì†ë„ í–¥ìƒì´ ì—†ìŒ.**
        â†’ ë”°ë¼ì„œ, **W4A4(ê°€ì¤‘ì¹˜ 4ë¹„íŠ¸, í™œì„±í™”ê°’ 4ë¹„íŠ¸) ì¡°í•©ì´ ìµœì í™”ëœ ë°©ì‹.**
- **W4A4 ì–‘ìí™”ì—ì„œì˜ ë¬¸ì œì : Outlier(ì´ìƒì¹˜)**
    - **Diffusion ëª¨ë¸ì—ì„œëŠ” ê°€ì¤‘ì¹˜(W)ì™€ í™œì„±í™”ê°’(X) ì–‘ìª½ì—ì„œ Outlier(ê·¹ë‹¨ì ì¸ ê°’)ê°€ ë§ì´ ë°œìƒí•¨.**
    - **Outlierê°€ ë§ìœ¼ë©´ ì–‘ìí™” í›„ í’ˆì§ˆì´ í¬ê²Œ ì €í•˜ë¨.**
    - ê¸°ì¡´ í•´ê²° ë°©ë²•:
        1. **Quantization-Aware Training (QAT)**
            - **ì–‘ìí™”ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë°©ì‹.**
            - **í•˜ì§€ë§Œ, 100ì–µ ê°œ ì´ìƒì˜ ë§¤ê°œë³€ìˆ˜(ì˜ˆ: FLUX.1 ëª¨ë¸)ë¥¼ ì¡°ì •í•˜ë ¤ë©´ ê³„ì‚° ë¹„ìš©ì´ ë§¤ìš° í¼.**
        2. **Rotation ê¸°ë²•** (Ashkboos et al., 2024; Liu et al., 2024c)
            - **ê°€ì¤‘ì¹˜ì™€ í™œì„±í™”ê°’ì„ íšŒì „(rotation)í•˜ì—¬ Outlierë¥¼ ì¤„ì´ëŠ” ë°©ë²•.**
            - **í•˜ì§€ë§Œ, Diffusion ëª¨ë¸ì˜ "Adaptive Normalization Layer"ì—ì„œëŠ” ì ìš©ì´ ì–´ë ¤ì›€.**
            - ì´ìœ :
                - Adaptive Normalizationì€ **ì‹¤í–‰ ì‹œê°„(runtime) ì¤‘ì— ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±**.
                - ë”°ë¼ì„œ, **ì‚¬ì „ ê³„ì‚°ëœ íšŒì „ í–‰ë ¬ì„ ì ìš©í•  ìˆ˜ ì—†ìŒ.**
                - **ì‹¤í–‰ ì‹œê°„ì— íšŒì „ì„ ì ìš©í•˜ë©´ ì—°ì‚°ëŸ‰ì´ ì¦ê°€í•˜ì—¬ ì†ë„ê°€ ëŠë ¤ì§.**

> ğŸ“¢
> 
> **ì´ìƒì¹˜(Outlier)ê°€ ìˆìœ¼ë©´ ì–´ë–»ê²Œ ì„±ëŠ¥ì´ ì €í•˜ë ê¹Œ?**
> 
> ### **1. ìŠ¤ì¼€ì¼ë§ íŒ©í„° ë¬¸ì œ**
> 
> - ì–‘ìí™”ëŠ” ë°ì´í„°ì˜ ì „ì²´ ë²”ìœ„(min-max)ë¥¼ ê³ ë ¤í•´ì„œ ê°’ì„ ì¡°ì •í•´ì•¼ í•˜ëŠ”ë°, ì´ìƒì¹˜ê°€ ìˆìœ¼ë©´ **ìŠ¤ì¼€ì¼ë§ íŒ©í„°ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì»¤ì§**.
> - ëŒ€ë¶€ë¶„ì˜ ê°’ì€ ì‘ì€ ë²”ìœ„ì— ëª°ë ¤ ìˆëŠ”ë°, **í•œë‘ ê°œì˜ í° ê°’(ì´ìƒì¹˜) ë•Œë¬¸ì— ìŠ¤ì¼€ì¼ì´ ì»¤ì§€ë©´ ì‘ì€ ê°’ë“¤ì´ ëª¨ë‘ 0 ë˜ëŠ” ë™ì¼í•œ ê°’ìœ¼ë¡œ ë§¤í•‘ë˜ëŠ” ë¬¸ì œ**ê°€ ìƒê²¨.
> 
>     **ì˜ˆì œ:**
> 
>     - ì›ë˜ ê°€ì¤‘ì¹˜ ê°’: `[-0.1, -0.05, 0.0, 0.05, 0.1, 5.0]` (ì´ìƒì¹˜: 5.0)
>     - ì´ìƒì¹˜ê°€ ì—†ì„ ë•Œ: `s_X = 0.1`, ë²”ìœ„ë¥¼ `[-8, 7]`ë¡œ ë§¤í•‘ ê°€ëŠ¥
>     - ì´ìƒì¹˜(5.0)ê°€ í¬í•¨ë  ë•Œ: `s_X = 5.0`, ì‘ì€ ê°’ë“¤ì€ ëª¨ë‘ 0ì´ ë˜ì–´ ì •ë³´ ì†ì‹¤ ë°œìƒ
> 
> ### **2. ì •ë³´ ì†ì‹¤ (Precision Loss)**
> 
> - ì´ìƒì¹˜ë¥¼ ê³ ë ¤í•´ ì „ì²´ ê°’ì„ ì¡°ì •í•˜ë©´, ë‚˜ë¨¸ì§€ ëŒ€ë¶€ë¶„ì˜ ê°’ì´ **ë§¤ìš° ì‘ì€ ì°¨ì´ë¥¼ ê°€ì§€ëŠ”ë°ë„ ë™ì¼í•œ ì–‘ìí™”ëœ ê°’ìœ¼ë¡œ í‘œí˜„**ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„.
> - ì¦‰, **ëª¨ë¸ì´ ì‘ì€ ë³€í™”(gradient ë“±)ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ê³  í‘œí˜„ë ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§**.
> 
> ### **3. í™œì„±í™”(Activation) ì´ìƒì¹˜ë¡œ ì¸í•´ ì—°ì‚°ëŸ‰ ì¦ê°€**
> 
> - ì´ìƒì¹˜ê°€ ìˆìœ¼ë©´ ì–‘ìí™”ëœ ê°’ì„ ë‹¤ì‹œ ë¶€ë™ì†Œìˆ˜ì ìœ¼ë¡œ ë³€í™˜í•  ë•Œ **FP32(32-bit)ë¡œ ë³€í™˜í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„**, ê²°êµ­ ì—°ì‚° ìµœì í™”ê°€ ê¹¨ì§.
> - íŠ¹íˆ Transformer ê¸°ë°˜ ëª¨ë¸ì—ì„œëŠ” Self-Attention ì—°ì‚°ì´ í¬ê¸° ë•Œë¬¸ì— **í™œì„±í™”ê°’(Activation)ì˜ ì´ìƒì¹˜ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì—°ì‚°ëŸ‰ ì¦ê°€ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŒ**.
> 
{: .block-warning }

# 4 Method

## 4.1 PROBLEM FORMULATION

ì–‘ìí™”ì˜ ì˜¤ë¥˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë¨.

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%207.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

ì›ë˜ í–‰ë ¬ ê³±ì…ˆ XWì™€ ì–‘ìí™”ëœ ê°’ìœ¼ë¡œ ì—°ì‚°í•œ Q(X)Q(W)ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ê°’

ì¢€ ë” ì„¸ë¶„í™”

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%208.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%209.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

## 4.2 SVDQUANT: ABSORBING OUTLIERS VIA LOW-RANK BRANCH

### Migrate outliers from activation to weight

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2010.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

After Smoothing ë¶€ë¶„ì´ ê¸°ì¡´ ê¸°ë²•ì¸ë°, ë‹¨ì ì´ ìˆìŒ

- âœ… **Activation(X)ì˜ ì´ìƒì¹˜ë¥¼ ì—†ì• ëŠ” ê²ƒì€ ì„±ê³µí–ˆì§€ë§Œ**,
- âŒ **ëŒ€ì‹  Weight(W)ì˜ ì´ìƒì¹˜ê°€ ì¦ê°€í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒ**.
- ê²°ê³¼ì ìœ¼ë¡œ, **ì „ì²´ì ì¸ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ì¤„ì´ë ¤ëŠ” ëª©ì ì´ ì œëŒ€ë¡œ ë‹¬ì„±ë˜ì§€ ì•ŠìŒ**.

### Absorb magnified weight outliers with a low-rank branch.

**Weightë¥¼ ë°”ë¡œ 4-bitë¡œ ì–‘ìí™”í•˜ì§€ ì•Šê³ , Low-Rank Componentë¥¼ ë”°ë¡œ ë¶„ë¦¬í•´ì„œ ì´ìƒì¹˜ë¥¼ í¡ìˆ˜**í•˜ëŠ” ì „ëµ

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2011.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2012.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

### SVD(Singular Value Decomposition, íŠ¹ì´ê°’ ë¶„í•´)

- ì›ë˜ í–‰ë ¬ í¬ê¸°ê°€ mÃ—nì´ë©´, ì§ì ‘ ê³±í•˜ë©´ ì—°ì‚°ëŸ‰ì´ **O(mn)**.
- í•˜ì§€ë§Œ SVDë¡œ Rank rë§Œ ìœ ì§€í•˜ë©´ **ì—°ì‚°ëŸ‰ì´ O(mr+rn)ë¡œ ì¤„ì–´ë“¦**.

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2013.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

 í¬ì¸íŠ¸ëŠ” ëŒ€ê° ì›ì†Œ(íŠ¹ì´ê°’, Singular Values)

- **í° íŠ¹ì´ê°’ë“¤ì€ ì¤‘ìš”í•œ ì •ë³´(íŒ¨í„´)ë¥¼ ë‚˜íƒ€ëƒ„**.
- **ì‘ì€ íŠ¹ì´ê°’ë“¤ì€ ë…¸ì´ì¦ˆ(ì´ìƒì¹˜ í¬í•¨)ë¥¼ ë‚˜íƒ€ë‚¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ**.

### Low-Rank ë¶„í•´

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2014.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

**ëŒ€ê° ì›ì†Œ ì¤‘ì—ì„œ ìƒìœ„ rê°œì˜ íŠ¹ì´ê°’ë§Œ ìœ ì§€í•˜ì—¬, ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë§Œ í¬í•¨í•˜ëŠ” L1,L2ë¥¼ ìƒì„±.** 

**â†’ ë”°ë¡œ 16-bit ì—°ì‚°**

**ë‚¨ì€ ë¶€ë¶„(ì‘ì€ íŠ¹ì´ê°’) â†’ Rìœ¼ë¡œ ë¶„ë¦¬** 

**â†’ 4-bit ì—°ì‚°**

â†’ ì‘ì€ íŠ¹ì´ê°’ë§Œ ë‚¨ì•˜ìœ¼ë¯€ë¡œ Rì„ **4-bitë¡œ ì–‘ìí™”í•˜ë”ë¼ë„ ì •ë³´ ì†ì‹¤ì´ í¬ê²Œ ì¤„ì–´ë“¦**

## 4.3 NUNCHAKU: Fusing Low-Rank and Low-Bit Branch Kernels

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2015.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

### **Low-Rank Branchì—ì„œ ë°œìƒí•˜ëŠ” ì„±ëŠ¥ ì €í•˜ ë¬¸ì œ**

- **QKV Projectionê³¼ ê°™ì€ ì—°ì‚°ì—ì„œëŠ” Low-Rank Branchê°€ L2 ìºì‹œë¥¼ ì´ˆê³¼í•˜ë©´ì„œ DRAMì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì•¼ í•¨**.
- ì´ëŠ” **ë©”ëª¨ë¦¬ ì ‘ê·¼ ë¹„ìš©ì´ ì¦ê°€í•˜ì—¬ ì—°ì‚° ì†ë„ê°€ ë–¨ì–´ì§€ëŠ” ì›ì¸**.
- **Figure 6(a)ì—ì„œ ë³´ë“¯ì´, Low-Rank BranchëŠ” ì „ì²´ 4-bit ì—°ì‚° ì§€ì—°ì˜ 50%ë¥¼ ì°¨ì§€**.

### **NUNCHAKU: í•´ê²° ë°©ë²•**

- ë…¼ë¬¸ì—ì„œëŠ” **Low-Rank Branchì™€ Low-Bit Branchì˜ ì—°ì‚°ì„ í•˜ë‚˜ë¡œ í•©ì³(fusing) ë©”ëª¨ë¦¬ ì ‘ê·¼ì„ ì¤„ì´ëŠ” ë°©ë²•ì„ ì œì•ˆ**.
- **Figure 6(b)ì—ì„œ ë³´ë“¯ì´, ë‘ ê°œì˜ Kernelì„ í•©ì³ì„œ ë°ì´í„°ë¥¼ ê³µìœ í•¨**:
    1. **Down Projection ì—°ì‚°ì„ Quantization Kernelê³¼ í•©ì¹¨**.
    2. **Up Projection ì—°ì‚°ì„ 4-bit ì—°ì‚° Kernelê³¼ í•©ì¹¨**.
- ì´ë¥¼ í†µí•´ **Low-Rank Branchê°€ Low-Bit Branchì™€ í™œì„±í™”ê°’ì„ ê³µìœ í•  ìˆ˜ ìˆì–´, ì¶”ê°€ì ì¸ ë©”ëª¨ë¦¬ ì ‘ê·¼ì„ ì œê±°**.
- ê²°ê³¼ì ìœ¼ë¡œ, **Kernel í˜¸ì¶œ íšŸìˆ˜ê°€ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“¤ì–´ ì†ë„ ê°œì„  íš¨ê³¼ê°€ ìˆìŒ**.

# 5 Experiments

### Benchmark models

| Model | Architecture | Parameters | Special Features |
| --- | --- | --- | --- |
| FLUX.1-dev | DiT | 12B | 50-step guidance-distilled |
| FLUX.1-schnell | DiT | 12B | 4-step timestep-distilled |
| PixArt-Î£ | DiT | 600M | 20-step default |
| SANA | DiT | 1.6B | 32Ã— compression autoencoder, Linear Attention |
| SDXL | UNet | 2.6B | 30-step |

### Baselines Quantization

| Method | Description | Usage in Benchmarking |
| --- | --- | --- |
| NF4<br>(4-bit NormalFloat) | Optimized 4-bit weight-only quantization assuming normal distribution | Used as a weight-only quantization baseline for FLUX.1 |
| ViDiT-Q | Per-token quantization + smoothing to reduce outliers | Achieves lossless 8-bit quantization on PixArt-Î£ |
| MixDQ | Detects outliers in text embeddings and protects them with 16-bit pre-computation | Enables W4A8 quantization with minimal performance drop on SDXL-Turbo |
| TensorRT | Industry-standard PTQ toolkit for 8-bit quantization | Uses smoothing + percentile calibration over specific timesteps |

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2016.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2017.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2018.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2019.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2020.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

# Limitation

ì—„ì²­ë‚œ ê¸°ìˆ ì´ê³  ê¸°ìˆ ë©´ì—ì„œëŠ” í•œê³„ê°€ ì—†ë‹¤ê³  ìƒê°í•¨. 

í•˜ì§€ë§Œ, êµ³ì´ í•œê³„ì ì„ ë½‘ìë©´, Song Hanì´ NVIDIAì—ì„œë„ ì—°êµ¬ë¥¼ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— NVIDIA chipë§Œì„ ìœ„í•´ì„œ ì½”ë“œë¥¼ ì§°ê³ , ì´ì— ìµœì í™”ë˜ì–´ìˆë‹¤.

ì‹¬ì§€ì–´ CUDA 12.2 ì´ìƒì—ì„œë§Œ ì‘ë™ ê°€ëŠ¥í•´ì„œ ë‚´ í•™êµ ì„œë²„ë¡œ ëŒë ¤ë³´ë ¤ê³  í–ˆëŠ”ë°, GPU Driver versionì´ ë‚®ì•„ì„œ ì•ˆëŒì•„ê°€ë”ë¼.

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2021.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

ë¬¼ë¡  NVIDIA chipì—ì„œ ê·¹í•œì˜ ìµœì í™”ë¥¼ ìœ„í•´ì„œ ì˜€ì§€ë§Œ, ë‹¤ë¥¸ GPU ì¥ë¹„ì—ì„œëŠ” ì´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

ê°™ì€ ë°©ì‹ì„ ë‹¤ë¥¸ GPU ì¥ë¹„ì™€ Mobile edge deviceë“¤ì— ì ìš©í•œë‹¤ë©´ ì¢‹ì„ ê²ƒì´ë‹¤.


# ì§ˆë¬¸ê³¼ ë‹µë³€

## ë©”ëª¨ë¦¬ë‘ ì¶”ë¡ ì‹œê°„ë§Œ ì¤„ì¸ê²Œ ì•„ë‹Œê°€? ì •í™•ë„ë¥¼ ì™œ ì–¸ê¸‰?

### Memory, Latency?

ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ SVDì–‘ìí™”ë¡œ ì¸í•´ ë©”ëª¨ë¦¬ì™€ latency ì´ì ì„ ì–»ì€ê²Œ í¬ì¸íŠ¸ ì•„ë‹Œê°€?

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2022.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

ë¹„êµê°€ ê¸°ì¡´ 16bit / W4A16 / W4A4(SVD) ì˜€ê¸° ë•Œë¬¸ì— í° ì°¨ì´ë¥¼ ë³´ì—¬ì¤€ ê²ƒ ê°™ë‹¤.

Inference timeê³¼ Memory ì¤„ì¸ê²Œ í¬ì¸íŠ¸ì¸ì¤„ ì•Œì•˜ëŠ”ë° ì™„ì „ ì˜ëª» ìƒê°í•œê²ƒ ê°™ê¸°ë„ í•©ë‹ˆë‹¤.

ë¬¼ë¡  Outlierë•Œë¬¸ì— 32bitë¡œ ì²˜ë¦¬í–ˆë˜ ë¶€ë¶„ë“¤ì´ ì—†ì–´ì§€ê³  16bitë¡œ low rankë¡œ ë”°ë¡œ ë¹¼ë‹ˆê¹Œ í–¥ìƒì€ ëì„ ê²ƒì´ì§€ë§Œ, ì œ ìƒê°ì—ëŠ” SVD ì—†ëŠ” W4A4ë‘ ë¹„êµí–ˆë‹¤ë©´, Memoryì™€ ì¶”ë¡ ì‹œê°„ì´ 3ë°° ì´ìƒ ì°¨ì´ë‚˜ì§€ëŠ” ì•Šì„ ê²ƒì…ë‹ˆë‹¤.

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2023.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

### ì •í™•ë„?

ê¸°ì¡´ ì–‘ìí™”ì—ì„œ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ëŠ” ë¶€ë¶„ì´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ë§ì´ ë–¨ì–´ì¡Œì—ˆë‹¤.

(ë‘ë²ˆì§¸ê°€ ê¸°ì¡´ ì–‘ìí™” ê¸°ë²•, ì‹¬ì§€ì–´ W4A4ê°€ ì•„ë‹Œ W4A16ì¸ë°ë„ ë” ë–¨ì–´ì§€ëŠ” ëª¨ìŠµì„ ë³´ì„)

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2024.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

### ë³´ì¡´ í•  ìˆ˜ ìˆì—ˆë˜ ì´ìœ  â†’ Low-Rank Branch

- ê¸°ì¡´ 4-bit ì–‘ìí™” ë°©ì‹ì—ì„œëŠ” **Weight ì „ì²´ë¥¼ 4-bitë¡œ ë³€í™˜**í•˜ë¯€ë¡œ ì •ë³´ ì†ì‹¤ì´ í¼.
- SVDQuantëŠ” Weightë¥¼ Low-Rank Component (L1L2) ì™€ ì”ì—¬ (R)ë¡œ ë¶„í•´í•œë‹¤.

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2025.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

- L1L2ëŠ” 16-bit precisionìœ¼ë¡œ ìœ ì§€ â†’ ì¤‘ìš”í•œ ì •ë³´ëŠ” ê³ ì •ë°€ë„ë¡œ ë‚¨ê²¨ë‘ .
- **ì”ì—¬ Rë§Œ 4-bitë¡œ ì–‘ìí™”í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•¨**
- í•œ ë²ˆë§Œ Low-Rank ë¶„í•´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ë°˜ë³µì ìœ¼ë¡œ Rì„ ìµœì í™”**í•˜ì—¬ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”

### quantization error

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2026.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

ì´ ì‹ì—ì„œ L1L2+Rë¡œ ë¶„í•´í•¨ìœ¼ë¡œì„œ Quantizationì„ ì§„í–‰í•˜ë©´ì„œ ë°œìƒí•˜ëŠ” Errorë¥¼ ìµœëŒ€í•œ ì¤„ì¸ê±°ì„.

Quantization ìì²´ê°€ Outlierë¡œ ì¸í•´ì„œ ì •í™•ë„ë¥¼ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ë°–ì— ì—†ëŠ”ë°, ì´ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í–ˆë‹¤ëŠ” ì ì´ ì—„ì²­ë‚œ ì—°êµ¬ì¸ ê²ƒì„!!!!

## SVDë¥¼ LLMì— ì¨ë„ ë˜ëŠ”ê°€? ì™œ Diffusionìœ¼ë¡œ ë…¼ë¬¸ì„?

ê¸°ì¡´ ì œ ìƒê° : ì ìš© í•  ìˆ˜ëŠ” ìˆì„ ê²ƒ ê°™ìœ¼ë‚˜, LLMì€ ë³‘ëª©í˜„ìƒì´ ë¬´ê±°ìš´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •ì—ì„œ ë‚˜íƒ€ë‚˜ë¯€ë¡œ ë’¤ì— ì—°ì‚°ì„ ì¤„ì—¬ë„ Diffusionë§Œí¼ í° íš¨ê³¼ëŠ” ë‚˜íƒ€ë‚  ì§€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤. (Diffusionì€ ì—°ì‚°ì´ ë³‘ëª©ì„)

ë¬¼ë¡  ê¸°ì¡´ ì œ ìƒê°ë„ ì°¾ì•„ë³´ë‹ˆ ë§ëŠ” ê²ƒ ê°™ìœ¼ë‚˜, ì‹¤ì œë¡œ í™œìš©í•œë‹¤ë©´, ë°”ë¡œ ìœ„ ì§ˆë¬¸ì—ì„œ ë‹¤ë¤˜ë˜ ì •í™•ë„ í–¥ìƒì—ë„ ë„ì›€ì´ ë˜ê¸° ë•Œë¬¸ì—, ì˜¤íˆë ¤ ì •í™•ë„ ë¶€ë¶„ì—ì„œ ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

### ê¸°ì¡´ ì–‘ìí™”

- **GPTQ** â†’ Post-training quantization ë°©ì‹, Weightë§Œ 4-bit ë³€í™˜.
- **AWQ** â†’ Weight ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•´ ì„ íƒì ìœ¼ë¡œ 4-bit ë³€í™˜.
- **SmoothQuant** â†’ Activation ì´ìƒì¹˜ë¥¼ Weightë¡œ ì´ë™ì‹œì¼œ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ì¤„ì´ëŠ” ë°©ì‹.

### SVDQuant ì‚¬ìš©í•œë‹¤ë©´?

Weightë¥¼ Low-Rank(16-bit) + Residual(4-bit)ë¡œ ë‚˜ëˆ„ì–´ ì¤‘ìš”í•œ ì •ë³´ëŠ” ìœ ì§€í•˜ë©´ì„œ ì••ì¶•í•˜ë¯€ë¡œ

Diffusionì²˜ëŸ¼ ì •í™•ë„ í–¥ìƒì— ë„ì›€ì´ ë ë“¯ !!!

## For an explanation from the author, Song Han

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2027.webp" class="img-fluid rounded z-depth-1" zoomable=true %}

[Youtube [Introduction to SVDQuant for 4-bit Diffusion Models]](https://www.youtube.com/watch?v=nYujDH9r69s&t=1s)

## Demo

[[https://hanlab.mit.edu/projects/svdquant](https://hanlab.mit.edu/projects/svdquant)](https://hanlab.mit.edu/projects/svdquant)

{% include figure.liquid loading="eager" path="files/2025-03-06-svdquant/image%2028.webp" class="img-fluid rounded z-depth-1" zoomable=true %}