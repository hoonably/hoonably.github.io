---
layout: notion
title: "TTRL: Test-Time Reinforcement Learning"
description:
date: 2025-07-31 01:30:11 +09:00
tags: AI
categories: Paper
giscus_comments: true
related_posts: false

featured: false
pretty_table: true

toc:
  beginning: false  # 맨 앞에 목차
  sidebar: left  # 목차가 사이드바 왼쪽에 붙어있음
---
<table class="simple-table" id="23f451cf-7b79-80d5-ba3c-ee8bbfa08ffd"><tbody><tr id="23f451cf-7b79-80ae-9180-ffcfd13e000a"><th class="simple-table-header-color simple-table-header" id="MApI" style="width:125.5px">ArXiv</th><td class="" id="L|H:" style="width:580.5px"><a href="https://arxiv.org/abs/2504.16084">https://arxiv.org/abs/2504.16084</a></td></tr><tr id="23f451cf-7b79-802d-a5ea-ee3926e2f82e"><th class="simple-table-header-color simple-table-header" id="MApI" style="width:125.5px">Github Code</th><td class="" id="L|H:" style="width:580.5px"><a href="https://github.com/PRIME-RL/TTRL">https://github.com/PRIME-RL/TTRL</a></td></tr><tr id="23f451cf-7b79-8046-b211-ebd8ca0d2c04"><th class="simple-table-header-color simple-table-header" id="MApI" style="width:125.5px">Authors</th><td class="" id="L|H:" style="width:580.5px">Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, Bowen Zhou</td></tr><tr id="23f451cf-7b79-80dd-8748-e9bd04a6d391"><th class="simple-table-header-color simple-table-header" id="MApI" style="width:125.5px">Affiliation</th><td class="" id="L|H:" style="width:580.5px">Tsinghua University, Shanghai AI Lab</td></tr></tbody></table><figure class="block-color-teal_background callout" id="23f451cf-7b79-8013-9cec-eb52c92f1382" style="white-space:pre-wrap;display:flex"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"> <strong>Key Differentiator</strong><br/>(1) 다양한 답변을 생성하고 (Test-Time Scaling)<br/>(2) majority voting을 통해 "이 답변은 good, 이건 bad"라는 평가를 자동으로 생성<br/>(3) 이를 reward로 변환해 RL 수행<br/><mark class="highlight-default">Test Time에 </mark><mark class="highlight-red">자율적, 반복적, label-free </mark><mark class="highlight-default">방식으로 학습</mark>하고 더 좋은 결과를 내는 효과<br/><mark class="highlight-blue">답안지 안주고 문제만 줬는데, 알아서 반복적으로 문제풀면서 똑똑해진다!</mark></div></figure><p class="" id="241451cf-7b79-8076-ad27-c26847b7af24">
</p><figure class="block-color-gray_background callout" id="241451cf-7b79-80fa-b229-c30666fbb446" style="white-space:pre-wrap;display:flex"><div style="font-size:1.5em"><span class="icon">🤷</span></div><div style="width:100%">Why I chose this paper?<li style="list-style-type:disc">Test-Time 논문들을 많이 봤지만, Reinforcement Learning은 처음 들어봐서 궁금했다.</li><li style="list-style-type:disc">아카이브에만 있고, 최근에 제출된 논문인데, github star이 700개나 되어있어서 궁금했다.</li></div></figure><p class="" id="241451cf-7b79-80d1-b270-d25db6e9cd8b">

<div style="position:relative; width:100%; aspect-ratio:1214/749;">
  <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQARrBbkuRqM9EqKFgs3jAdleCsGjfMThhlSn8FygDT3Mt6UvdJqNL50-ce-NAEmA/pubembed?start=false&loop=false&delayms=3000"
    style="position:absolute; top:0; left:0; width:100%; height:100%;"
    frameborder="0"
    allowfullscreen="true"
    mozallowfullscreen="true"
    webkitallowfullscreen="true">
  </iframe>
</div>

</p><p class="" id="240451cf-7b79-8026-8478-d8919bd0068d">
</p><blockquote class="" id="241451cf-7b79-80eb-a84e-d40e74898049">기존의  <strong>Test-Time Scaling</strong>이나  <strong>Reinforcement Learning</strong>을 먼저 설명하는게 좋을 것 같아서<p class="" id="240451cf-7b79-80f8-b8da-d10107724a5a"> <strong>5 Related Works</strong>를 먼저 읽었다.</p></blockquote><h1 class="" id="23f451cf-7b79-8044-989f-d261c9456ba2">5 Related Works  </h1><h2 class="" id="23f451cf-7b79-8033-92a6-eac5f84bc160">5.1 Test-Time Scaling</h2><p class="" id="23f451cf-7b79-80a1-a4e2-fb2c87423d91">= LLM이 테스트(추론) 시점에서 더 많은 계산 자원을 사용해 성능을 높이는 방법</p><p class="" id="23f451cf-7b79-801b-8b2f-cb9270214b29">→ 즉, 학습된 모델 구조는 그대로 두고,  <strong>test-time에 inference 방식만 확장</strong>하는 전략</p><p class="" id="23f451cf-7b79-800f-9f45-da5854c20383">
</p><p class="" id="23f451cf-7b79-80cd-8c25-ee13e76d7d65">① Parallel Generation</p><p class="" id="23f451cf-7b79-8096-a88f-fe4a659d8e80">하나의 입력에 대해 여러 개의 output을 생성하고 그 중 “좋은 것”을 선택</p><li style="list-style-type:disc"> <strong>Self-Consistency</strong> (Wang et al., 2022)<li style="list-style-type:circle">여러 CoT 답변을 만들고 가장 많은 빈도를 가진 답변을 선택 (majority voting)</li></li><li style="list-style-type:disc"> <strong>Best-of-N</strong> (Stiennon et al., 2020; Nakano et al., 2021)<li style="list-style-type:circle">reward function이나 score function으로 best 답변 선택</li></li><li style="list-style-type:disc"> <strong>Reward-guided Search</strong> (Deng &amp; Raffel, 2023; Khanov et al., 2024)<li style="list-style-type:circle">sampling된 결과에 external reward function을 적용해 선택</li></li><p class="" id="23f451cf-7b79-8044-958d-e876f7f0fdfe">→ 이처럼 parallel하게 여러 답안을 만들고 하나를  <strong>“선택”하거나 “aggregation”</strong> 하는 게 공통</p><p class="" id="23f451cf-7b79-80e8-93f8-dfb589c80f15">
</p><p class="" id="23f451cf-7b79-80d4-bc81-c312689c822a">② Sequential Generation</p><p class="" id="23f451cf-7b79-807d-9ba1-f35787194528">하나의 답변을 길게, 점진적으로 확장하거나 수정하며 reasoning</p><li style="list-style-type:disc"> <strong>Chain-of-Thought (CoT)</strong> prompting (Wei et al., 2022)<li style="list-style-type:circle">중간 reasoning step을 명시적으로 유도</li></li><li style="list-style-type:disc"> <strong>Reflective reasoning</strong> (Madaan et al., 2023)<li style="list-style-type:circle">스스로 답을 검토하고 수정</li></li><p class="" id="23f451cf-7b79-80cb-bb1d-d12800e91d84">→ reasoning depth를 늘리거나 self-correction을 유도</p><p class="" id="23f451cf-7b79-80b6-8771-e2e6ce5cd7b5">
</p><p class="" id="23f451cf-7b79-806b-a3f5-ccdb4c5b35dd">한계: 대부분의 TTS는 prompt-based이며,  <strong>모델 파라미터 자체는 업데이트되지 않음</strong></p><p class="" id="23f451cf-7b79-8019-8025-ee1cb15a5c14">
</p><table class="simple-table" id="23f451cf-7b79-80b2-ab20-cac97ed3662a"><thead class="simple-table-header"><tr id="23f451cf-7b79-802c-b7a6-dde261647268"><th class="simple-table-header-color simple-table-header" id="dgVD" style="width:253.203125px">기존 TTS</th><th class="simple-table-header-color simple-table-header" id="?GWK" style="width:368.99998474121094px">TTRL</th></tr></thead><tbody><tr id="23f451cf-7b79-80d5-b628-dbb22f086d79"><td class="" id="dgVD" style="width:253.203125px">inference time에만 사용</td><td class="" id="?GWK" style="width:368.99998474121094px">inference + parameter update (TTT) 포함</td></tr><tr id="23f451cf-7b79-80af-8e56-fa25a34d2c88"><td class="" id="dgVD" style="width:253.203125px">majority voting만 사용</td><td class="" id="?GWK" style="width:368.99998474121094px">majority voting → reward로 전환하여 RL 수행</td></tr><tr id="23f451cf-7b79-80e3-8c35-ecc7053d19e1"><td class="" id="dgVD" style="width:253.203125px">non-parametric</td><td class="" id="?GWK" style="width:368.99998474121094px">parametric update 포함</td></tr></tbody></table><p class="" id="23f451cf-7b79-80fa-8a1b-fd0b7206516d">
</p><h2 class="" id="23f451cf-7b79-80df-960a-f10bab20ab52">5.2 RL for Reasoning</h2><p class="" id="23f451cf-7b79-8008-aba5-e9d6d24d7450">
</p><p class="" id="23f451cf-7b79-80ec-811a-c889338e0cdc"> <strong>Human Preference 기반</strong></p><ol class="numbered-list" id="23f451cf-7b79-807e-a9dc-f245579ed6f2" start="1" type="1"><li>Human 또는 annotator가 여러 답 중 선호도를 매김</li></ol><ol class="numbered-list" id="23f451cf-7b79-80a9-8976-df4e7934194c" start="2" type="1"><li>Preference Model 학습 → reward로 사용</li></ol><ol class="numbered-list" id="23f451cf-7b79-809d-b843-c86c5017dc1a" start="3" type="1"><li>PPO 등으로 policy (LLM) 업데이트</li></ol><li style="list-style-type:disc">강점: 자연언어적 open-ended instruction에는 적합</li><li style="list-style-type:disc">한계:  <strong>사람의 label이 필요</strong>하고,  <strong>수치적 평가 불가능한 domain</strong>에서만 가능</li><p class="" id="23f451cf-7b79-8049-9b2a-ed686d3173af">
</p><p class="" id="23f451cf-7b79-80c5-acf4-f617b87f179c"> <strong>Rule-based Reward 기반</strong></p><p class="" id="23f451cf-7b79-80a9-8004-d20bba2d0799">reasoning domain (예: 수학)에서는 정답을 명확하게 판별할 수 있음</p><p class="" id="23f451cf-7b79-8058-a479-ec3199ba3775">→ 맞았으면 reward = 1, 틀렸으면 0 같은  <strong>rule-based reward</strong> 사용 가능</p><p class="" id="23f451cf-7b79-8017-a9d6-d26d14787daa">
</p><p class="" id="23f451cf-7b79-805a-a6ad-cf611db203ff"> <strong>GRPO (Group Relative Policy Optimization)</strong>:</p><p class="" id="23f451cf-7b79-80d7-b457-e0378ef2a6d8">DeepSeek-R1에서 사용. 수학 문제에 대해 긴 CoT 생성 유도</p><p class="" id="23f451cf-7b79-8022-be2a-dbb9bc4c8417">PPO도 사용되지만, 수치적 reward의 안정성과 gradient variance가 문제됨</p><p class="" id="23f451cf-7b79-80f0-9c70-e92f46a01e81">
</p><table class="simple-table" id="23f451cf-7b79-8022-8cce-f248269d8bc0"><thead class="simple-table-header"><tr id="23f451cf-7b79-80cc-8860-f94665397978"><th class="simple-table-header-color simple-table-header" id=":{rp" style="width:121.6875px">구분</th><th class="simple-table-header-color simple-table-header" id="d@~H">RLHF</th><th class="simple-table-header-color simple-table-header" id="|Fw|" style="width:172.4375px">GRPO / Rule-based RL</th><th class="simple-table-header-color simple-table-header" id="Qb=S" style="width:169.2890625px">TTRL</th></tr></thead><tbody><tr id="23f451cf-7b79-8050-803e-e70b82cf0fa2"><td class="" id=":{rp" style="width:121.6875px">supervision source</td><td class="" id="d@~H">human preference</td><td class="" id="|Fw|" style="width:172.4375px">rule-based labels <br/>(정답존재)<br/></td><td class="" id="Qb=S" style="width:169.2890625px"> <strong>majority voting</strong> <br/>(pseudo-label)<br/></td></tr><tr id="23f451cf-7b79-80a0-a5c5-fcdec2ab6d1c"><td class="" id=":{rp" style="width:121.6875px">label 필요 여부</td><td class="" id="d@~H">필요</td><td class="" id="|Fw|" style="width:172.4375px">필요</td><td class="" id="Qb=S" style="width:169.2890625px"> <strong>불필요 (label 없음)</strong></td></tr><tr id="23f451cf-7b79-8056-b43b-d981996bdb42"><td class="" id=":{rp" style="width:121.6875px">학습 시점</td><td class="" id="d@~H">offline RL</td><td class="" id="|Fw|" style="width:172.4375px">offline RL</td><td class="" id="Qb=S" style="width:169.2890625px"> <strong>Test-time (online RL)</strong></td></tr><tr id="23f451cf-7b79-80bf-bd04-ea80e41d363c"><td class="" id=":{rp" style="width:121.6875px">task</td><td class="" id="d@~H">open-domain instruction</td><td class="" id="|Fw|" style="width:172.4375px">math, logic, program</td><td class="" id="Qb=S" style="width:169.2890625px">math, logic, program</td></tr></tbody></table><p class="" id="241451cf-7b79-806c-b521-c5d44a654156">
</p><p class="" id="241451cf-7b79-8005-9b3e-c9ab4028de22">
</p><hr id="23f451cf-7b79-80cb-a4c8-ceb48ca34901"/><h1 class="" id="23f451cf-7b79-8060-9397-c7cadf70a7e0">2. Test-Time Reinforcement Learning (TTRL)</h1><blockquote class="" id="23f451cf-7b79-8098-b1f7-ff2cdc96f014">We study the problem of training a pre-trained model during test time using RL without ground-truth labels. We call this setting Test-Time Reinforcement Learning.</blockquote><p class="" id="240451cf-7b79-80c8-80f9-c687549fa84f">
</p><h2 class="" id="23f451cf-7b79-80bf-bbb8-c9d97d99d929">2.1 Methodology</h2><figure class="image" id="23f451cf-7b79-806a-9b7e-dbaa8db953f7"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image.webp" style="width:709.9715576171875px"/></picture></figure><p class="" id="23f451cf-7b79-80b2-9528-c205bc70d15b">녹색 배경이 TTS + 이후 나온 결과로 reward calaulation을 통해 Test-Time에 Training</p><p class="" id="240451cf-7b79-805e-a015-e91b2dc1b750">M: 한 문제(q)에 대해 생성하는  <strong>답변 수</strong></p><p class="" id="240451cf-7b79-800d-9b93-e29aeea82944">N = batch_size(하나의 학습 step에서 사용하는  <strong>문제 수)</strong></p><p class="" id="240451cf-7b79-80b8-b020-ce347a6beebf">첫번째 문제에 M개의 답변 내고, voting하고 reward 계산해서 모아놓은게 R(y1, y)</p><p class="" id="23f451cf-7b79-80f7-81e9-f7f74a50a932">
</p><h3 class="" id="23f451cf-7b79-8023-b22e-e40da8d0d0d7">상태(state)와 행동(action)</h3><li style="list-style-type:disc">주어진 문제(prompt) x를 상태(state)로 보고,</li><li style="list-style-type:disc">LLM은 그에 대한 답변 y를  <strong>policy </strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span class="notion-text-equation-token" contenteditable="false" data-token-index="0" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\theta}(y \mid x)
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span>로부터 생성 (sampling)</li><p class="" id="23f451cf-7b79-80bf-9a18-de6f9fb17744">→  <strong>LLM의 답변 행위 = RL의 action</strong></p><p class="" id="23f451cf-7b79-80cd-a504-e3c6dcaea997">
</p><h3 class="" id="23f451cf-7b79-8028-8c1f-e6223b724e26">Rollout: 답변 여러 개 생성</h3><figure class="image" id="240451cf-7b79-8043-8848-f09b06b6c981"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%201.webp" style="width:709.9928588867188px"/></picture></figure><p class="" id="23f451cf-7b79-8020-b683-f5d4bae8b936">Ground-truth label이 없이 reward signal을 보내야하니까,  <strong>여러개의 candidate(후보) output을 생성</strong></p><p class="" id="23f451cf-7b79-80a4-897e-c2a8ef3e923b">→ <mark class="highlight-red_background">x에 대해 답변 {y1,...,yM} 을 sampling</mark> </p><p class="" id="240451cf-7b79-80c0-aa38-c3559d8b9ec3">랜덤성 있는 샘플링으로  <strong>M=64</strong>개의 다양한 답변 생성(appendix에서는 16)</p><p class="" id="23f451cf-7b79-807c-9a25-e4cf1ca78dae">→ 단일 답이 아닌 다양한 reasoning path를 확보해야 voting이 의미 있음</p><p class="" id="241451cf-7b79-80d9-92fd-cfb2d1631f29">
</p><li><details><summary>여러개의 답변은 자동으로 랜덤되는것인가? 따로 설정을 바꿔주는것인가?</summary><p class="" id="23f451cf-7b79-807e-ac23-f61a4362df9b">원래 LLM은 같은 입력에 대해서도 랜덤하게 다르게 나오긴 함.</p><p class="" id="23f451cf-7b79-8026-b7ab-d85cef1bfb6e">파라미터를 일부러 변화시키진 않고,  <strong>Randomized decoding 설정</strong></p><p class="" id="23f451cf-7b79-802e-b82a-cda6ca3dc509">→ temperature, top-p, top-k, sampling 횟수 등으로 조절</p><figure class="image" id="23f451cf-7b79-8014-914d-da89d38eec5f"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%202.webp" style="width:672px"/></picture></figure><li style="list-style-type:disc"><code>temperature = 0.6</code></li><li style="list-style-type:disc"><code>top-p = 0.95</code></li><p class="" id="23f451cf-7b79-8073-bea1-d393e8b776f4">→ sampling된 답변이 다르게 나오도록 유도</p><p class="" id="23f451cf-7b79-8018-ad0d-ce9c89ba386c">
</p><blockquote class="" id="23f451cf-7b79-8091-b236-d5d02f1fdc6a">Temperature: Setting the temperature to 1.0, as opposed to 0.6, increases the model’s output entropy. This promotes more extensive exploration and allows the model to make better use of its prior knowledge for self-improvement, which is particularly important when addressing challenging benchmarks.</blockquote><p class="" id="23f451cf-7b79-8065-a0b4-cd5d24e95c61">실제로 이후 실험에서 Parameter에 따른 비교가 있고, </p><p class="" id="241451cf-7b79-80f1-a257-d2b750388110">Dataset 난이도에 따라서 조정이 필요하다는 한계를 밝힘.</p><p class="" id="241451cf-7b79-80d7-adf4-c620fca26c46">(Figure 11 : Inappropriate RL Hyperparameters)</p><p class="" id="241451cf-7b79-8095-b24f-fd52a4df6557">어려운 task에 대해서는 Temperature을 1.0으로 해야 효과가 좋음.</p><p class="" id="241451cf-7b79-804a-9854-e9cc681a5e3e">→ temperature 높이면 diversity 증가 → exploration 증가 → high entropy → 다양한 답변</p><p class="" id="23f451cf-7b79-80f2-b4b5-c4de0e277aea">
</p></details></li><p class="" id="240451cf-7b79-80d8-a5f5-c3286a44d0ad">
</p><p class="" id="23f451cf-7b79-802a-b5dc-ca4a4f4e05a2">
</p><h3 class="" id="23f451cf-7b79-8032-ba84-e5a1a9510f85">정답 추출 + Majority Voting (Label 추정)</h3><figure class="image" id="240451cf-7b79-8057-94a4-cf9ec872350d"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%203.webp" style="width:709.9928588867188px"/></picture></figure><li style="list-style-type:disc">각 𝑦i에서 최종 정답만 extract → 숫자, 선택지</li><li style="list-style-type:disc"><mark class="highlight-blue"><mark class="highlight-default_background"> <strong>majority voting (다수결)로 가장 많이 나온 답을 pseudo-label 𝑦∗로 정함</strong></mark></mark></li><p class="" id="240451cf-7b79-8088-9b33-d79764dc2134">
</p><p class="" id="240451cf-7b79-8081-8bde-d6e06d9ea4c6">
</p><h3 class="" id="23f451cf-7b79-807c-b921-f32ebc0bd18e">Reward 계산</h3><figure class="image" id="240451cf-7b79-8024-9887-c5d632ae7760"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%204.webp" style="width:709.9928588867188px"/></picture></figure><p class="" id="23f451cf-7b79-8020-af8c-f5f68a8f8c50">sampling된 y답이 majority 답이랑 일치하면 → reward = 1</p><p class="" id="240451cf-7b79-8043-a908-d53df570677f">아니면 → reward = 0</p><figure class="image" id="23f451cf-7b79-8085-b276-c05356788692" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%205.webp" style="width:240px"/></picture></figure><p class="" id="23f451cf-7b79-8080-90ab-c9a5d3d5dfb2">→ 실제 정답을 모르지만, voting 결과에 얼마나 일치했는지를 기준으로 학습 신호 제공</p><p class="" id="240451cf-7b79-80f5-b8ab-faabdc2bc747">
</p><blockquote class="" id="240451cf-7b79-80f8-8763-d3d4b32c050e">We sample 64 responses per prompt using the current model and randomly select 32 to use for training.</blockquote><p class="" id="240451cf-7b79-8087-9b62-d8db511ddca6">랜덤하게 32개를 트레이닝에 사용그 중 32개만 골라서 reward 계산에 사용</p><p class="" id="240451cf-7b79-8062-9f2c-c77132b35c3a">→ 투표는 64개로 하는데, 나중에 RL은 랜덤으로 반만 사용함. (너무 계산 과도하니까)</p><p class="" id="240451cf-7b79-80ba-9977-f22b53cc81e8">
</p><figure class="block-color-teal_background callout" id="240451cf-7b79-80ad-b82c-d67dc448a2fa" style="white-space:pre-wrap;display:flex"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">위 과정을 batch size (N) 만큼 반복함. 매 질문마다 업데이트 하는게 아님.<blockquote class="" id="240451cf-7b79-803d-9c86-fa2ad5ebe9f9">Each RL step samples a batch of questions and computes policy gradients using the pseudo-rewards from majority voting.</blockquote>→  <strong>각 step마다 여러 개의 질문(batch of questions)을 사용</strong><br/>→ 이게 곧 우리가 말하는 “batch size”에 해당<br/>
<br/>데이터셋마다 다르게 사용했음.<br/>AIME=80, AMC=30, MATH-500=10<br/>AIME는 어려우니까 여러번하고 업데이트해줘야 틀린 정보로 업데이트가 반복될 확률이 줄어듬.</div></figure><p class="" id="241451cf-7b79-80c7-b5d0-fcbdd80cad59">
</p><p class="" id="23f451cf-7b79-80d3-a7ef-fbe33ed33149">
</p><h3 class="" id="23f451cf-7b79-80e8-82ab-f2ba6cd22fbc">Policy 업데이트 (RL)</h3><figure class="image" id="240451cf-7b79-8016-a269-d63be4bcdbb4"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%206.webp" style="width:720px"/></picture></figure><p class="" id="240451cf-7b79-80ec-bb33-f04da0189a57">batch size만큼 반복하면 이제 모아놨던</p><p class="" id="23f451cf-7b79-8011-96c4-f92e87529bc2">목표: expected reward를 최대화하는 것 → 다수결이 옳다고 믿자!</p><figure class="equation" id="23f451cf-7b79-8009-b062-ec1a7bd1df43"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>θ</mi></munder><msub><mi mathvariant="double-struck">E</mi><mrow><mi>y</mi><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo fence="true">[</mo><mi>r</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><msup><mi>y</mi><mo>∗</mo></msup><mo stretchy="false">)</mo><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\max_{\theta} \mathbb{E}_{y \sim \pi_{\theta}(\cdot | x)} \left[ r(y, y^*) \right]</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.5021em;vertical-align:-0.7521em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3479em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7521em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7387em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;">]</span></span></span></span></span></span></div></figure><p class="" id="240451cf-7b79-80b3-af41-e9749458d9d1">reward가 높았던 답변 쪽으로 (gradient ascent)</p><p class="" id="240451cf-7b79-8033-ad24-dcd1fc420324">θ (모델 파라미터)를 업데이트</p><figure class="equation" id="23f451cf-7b79-808a-81da-f70482e06880"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>←</mo><mi>θ</mi><mo>+</mo><mi>η</mi><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><msub><mi mathvariant="double-struck">E</mi><mrow><mi>y</mi><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo fence="true">[</mo><mi>r</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><msup><mi>y</mi><mo>∗</mo></msup><mo stretchy="false">)</mo><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\theta \leftarrow \theta + \eta \nabla_{\theta} \mathbb{E}_{y \sim \pi_{\theta}(\cdot | x)} \left[ r(y, y^*) \right]</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3552em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7387em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;">]</span></span></span></span></span></span></div></figure><figure class="block-color-teal_background callout" id="23f451cf-7b79-805b-abd9-d45c1f8f209d" style="white-space:pre-wrap;display:flex"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%">LLM이 prompt에 대해 여러 답변을 생성하고, 그 중 다수결로 추정된 label과 얼마나 일치하는지를 reward로 삼아, LLM의 policy를 reinforcement learning으로 업데이트한다.</div></figure><p class="" id="240451cf-7b79-8003-9d36-ce9c78d260c5">
</p><p class="" id="240451cf-7b79-80b0-935c-cf0c179556e8">
</p><p class="" id="240451cf-7b79-8070-b63c-f9f9cc98e99e">
</p><h2 class="" id="23f451cf-7b79-80e3-940e-c68c1ba1f242">2.2 Majority Voting Reward Function</h2><figure class="image" id="23f451cf-7b79-8053-9b47-f3832c099906"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%207.webp" style="width:709.9928588867188px"/></picture></figure>

```python
from collections import Counter

def majority_voting_reward_fn(outputs):
    # 1. 정답 추출
    answers = [extract_answer(output) for output in outputs]
    
    # 2. 다수결로 label 추정
    counts = Counter(answers)
    majority_answer, _ = counts.most_common(1)[0]
    
    # 3. reward 계산 (일치 여부 기준)
    rewards = [1 if ans == majority_answer else 0 for ans in answers]
    return rewards
```
<p class="" id="241451cf-7b79-80e9-9c6e-c2cc5c77505f">
</p><p class="" id="241451cf-7b79-80cd-b3d6-c3b9409f74db">
</p><p class="" id="241451cf-7b79-805b-9ad4-c1fc388b6d26">
</p><hr id="23f451cf-7b79-807f-b369-c6b7a60d06bf"/><h1 class="" id="23f451cf-7b79-8029-9470-f589da896f7a">3 Experiments</h1><h2 class="" id="23f451cf-7b79-8024-8350-da5c0e1a2cc6">3.1 Experimental Setup</h2><table class="simple-table" id="23f451cf-7b79-8055-8974-f9f6cb6688ce"><thead class="simple-table-header"><tr id="23f451cf-7b79-8027-a552-f1e1f32d8b6b"><th class="simple-table-header-color simple-table-header" id="MvWh" style="width:107.99999237060547px">구성요소</th><th class="simple-table-header-color simple-table-header" id="mWXN" style="width:263px">설정 내용</th><th class="simple-table-header-color simple-table-header" id="~oyC" style="width:338px">이유</th></tr></thead><tbody><tr id="23f451cf-7b79-8063-a869-db694264b0eb"><td class="" id="MvWh" style="width:107.99999237060547px">Models</td><td class="" id="mWXN" style="width:263px">Qwen, LLaMA, Mistral, DeepSeek 등 다양한 scale의 LLM</td><td class="" id="~oyC" style="width:338px">pretrained + post-trained 모델 모두 사용<br/>→ TTRL이 전형적인 SFT 이후에도 작동 가능한지 검증<br/></td></tr><tr id="23f451cf-7b79-80fb-ba55-c8e1def6bb44"><td class="" id="MvWh" style="width:107.99999237060547px">Tasks</td><td class="" id="mWXN" style="width:263px">AIME 2024, AMC, MATH-500, GPQA</td><td class="" id="~oyC" style="width:338px">정답이 명확하고 채점 가능한 task 위주 선택</td></tr><tr id="23f451cf-7b79-805d-ae8f-e61000881d82"><td class="" id="MvWh" style="width:107.99999237060547px">Sampling</td><td class="" id="mWXN" style="width:263px">64개 생성, 32개 학습 사용</td><td class="" id="~oyC" style="width:338px">label estimation 신뢰도 확보 + 연산 효율 고려</td></tr><tr id="23f451cf-7b79-80a1-9ae2-ff0a75a84127"><td class="" id="MvWh" style="width:107.99999237060547px">Decoding</td><td class="" id="mWXN" style="width:263px">temp=0.6, top-p=0.95</td><td class="" id="~oyC" style="width:338px"></td></tr><tr id="23f451cf-7b79-8057-b356-d922c889d4ce"><td class="" id="MvWh" style="width:107.99999237060547px">RL Algorithm</td><td class="" id="mWXN" style="width:263px"> <strong>GRPO</strong>,  <strong>AdamW</strong>, Cosine schedule Learning rate:5 × 10⁻⁷</td><td class="" id="~oyC" style="width:338px">실험적으로 안정성과 sample-efficiency가 검증된 방식</td></tr><tr id="23f451cf-7b79-80ac-b455-dfdee0cc5eb4"><td class="" id="MvWh" style="width:107.99999237060547px">Max Length</td><td class="" id="mWXN" style="width:263px">3072 (일반), 32768 (LRM)</td><td class="" id="~oyC" style="width:338px">CoT처럼 길고 reasoning-heavy한 답변도 처리 가능하도록 설계</td></tr><tr id="23f451cf-7b79-80c0-8212-e9cbac7489b7"><td class="" id="MvWh" style="width:107.99999237060547px">Episodes</td><td class="" id="mWXN" style="width:263px">AIME=80, AMC=30, MATH-500=10</td><td class="" id="~oyC" style="width:338px">dataset 난이도와 크기에 맞춰 적절히 조정</td></tr></tbody></table><li><details><summary>Dataset 설명</summary><p class="" id="240451cf-7b79-8048-b1fb-e26b2b71d455">AIME 2024 - American Invitational Mathematics Examination</p><li style="list-style-type:disc">고등학교 상위권 대상 미국 수학 경시대회 (3-digit integer)</li><p class="" id="240451cf-7b79-80d4-969d-ccfa878870ed">AMC - American Mathematics Competitions</p><li style="list-style-type:disc">AIME보다 쉬운 단계의 선다형 수학 경시 문제 (5지선다 A~E)</li><p class="" id="240451cf-7b79-80b8-ac73-d8b5a0fc187b">MATH-500 - Open-source 수학 문제집에서 500개 추출</p><li style="list-style-type:disc">수식, 정수 → 프로그램으로 직접 계산 (symbolic checker 사용)</li><p class="" id="240451cf-7b79-8021-85f6-e39b93da7195">GPQA - Graduate-level Physics Question Answering</p><li style="list-style-type:disc">이 중 Diamond 난이도만 (객관식)</li><p class="" id="240451cf-7b79-8009-bd3a-c3830a27353c">
</p></details></li><p class="" id="240451cf-7b79-80d0-ad43-cda625c50c10">
</p><p class="" id="240451cf-7b79-80ff-a3de-e4ee31271329">
</p><h2 class="" id="23f451cf-7b79-8067-ae6d-f50c97867dda">3.2 Main Results</h2><h3 class="" id="240451cf-7b79-803f-bca0-d7fd3048a847">Table 1 : Performs well on most tasks</h3><figure class="image" id="23f451cf-7b79-8007-9e99-c348a3e0ab19" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%208.webp" style="width:528px"/></picture></figure><p class="" id="240451cf-7b79-800a-b717-e41bee762adc">Qwen2.5-Math-1.5B 같은 1.5B 모델이 73.0까지 가기도 함.<br/>→ 소형 모델은 RL이 어려웠다는 걸 깸<br/><br/>* 여기서는 Qwen3-8B가 non-thinking mode이고, thinking mode는 Figure 3<br/></p><p class="" id="240451cf-7b79-80fd-8787-c14bdf2b0f04">
</p><p class="" id="240451cf-7b79-806f-b423-eddca2305895">
</p><h3 class="" id="240451cf-7b79-8070-b30b-dc24b7ce5eda">Table 2 : Performs well on most models</h3><figure class="image" id="23f451cf-7b79-8069-94ac-c0a9464371eb" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%209.webp" style="width:432px"/></picture></figure><p class="" id="240451cf-7b79-8052-9cfb-dbfed28c498c">
</p><p class="" id="23f451cf-7b79-804b-8aaa-d0db412bd767">LLaMA-Instruct, DeepSeek-R1, Mistral 등 다양한 모델에서 테스트</p><p class="" id="240451cf-7b79-8022-9080-d701ee2dd05f">
</p><p class="" id="240451cf-7b79-8099-ac35-ed608668dc06">
</p><h3 class="" id="240451cf-7b79-8025-8ac4-ff07c4a07636">Figure 3 : TTRL performs well on LRMs</h3><figure class="image" id="23f451cf-7b79-80f1-9ddd-da1a2bc484b1" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2010.webp" style="width:336px"/></picture></figure><p class="" id="240451cf-7b79-8034-a11b-d095d2ffca52">Large Reasoning Models에도 좋은 성능을 보임</p><p class="" id="240451cf-7b79-8017-8a49-d6e755b3b307">이미 Reasoning쪽으로 타겟해서 학습한 모델도 향상됨</p><p class="" id="240451cf-7b79-80de-a168-db97fad92c46">
</p><p class="" id="240451cf-7b79-80ff-b5e5-e4995a375ca5">
</p><h3 class="" id="240451cf-7b79-8003-bb37-f5dfd44ec308">Figure 4 : TTRL generalizes well beyond the target task</h3><figure class="image" id="23f451cf-7b79-80b1-ab57-d20470accbac" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2011.webp" style="width:576px"/></picture></figure><p class="" id="240451cf-7b79-8044-be24-ee6e9c7e70d6">특정 벤치마크에서 학습 후 다른 task에서도 성능이 같이 올라감.</p><p class="" id="240451cf-7b79-80d5-86d2-d8da741b1da3">
</p><p class="" id="240451cf-7b79-80d6-9228-c14912ce4de7">
</p><h3 class="" id="240451cf-7b79-8022-93f7-d59233a11c6b">Figure 5 : TTRL is compatible with different RL algorithms</h3><figure class="image" id="240451cf-7b79-80bf-99fb-c7c0e6662478" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2012.webp" style="width:576px"/></picture></figure><p class="" id="240451cf-7b79-808f-ae8d-c6d789d6b4dd">GRPO, PPO, PRIME을 비교</p><p class="" id="241451cf-7b79-80d1-a6ea-eb3edc3c921c">GRPO (rule-based), PPO (value-based), PRIME (process-level reward) </p><p class="" id="241451cf-7b79-8030-bfab-c0c793b94aa3">→ 다른 알고리즘으로도 호환이된다.</p><p class="" id="241451cf-7b79-8058-84cd-e6ce74a8a8db">
</p><li><details><summary> <strong>PPO</strong> : Proximal Policy Optimization</summary><p class="" id="241451cf-7b79-802d-be42-c396c4c7d495">현재의 policy를 너무 많이 바꾸지 않으면서 조금씩 좋아지게</p><p class="" id="241451cf-7b79-8091-8f22-e88408788c3f">value function V(s) 을 사용해서 현재 상태가 얼마나 좋은지를 예측하고,</p><p class="" id="241451cf-7b79-80e4-a233-eacfd33f6fe8">그 기준으로 얼마나 정책을 바꿀지를 계산</p></details></li><p class="" id="241451cf-7b79-8003-9ed3-cfcb75e6667b">
</p><li><details><summary> <strong>PRIME</strong> : Process Reinforcement through Implicit Rewards</summary><p class="" id="241451cf-7b79-809f-bbe8-fc024bbc9fcb"><a href="https://arxiv.org/abs/2502.01456">https://arxiv.org/abs/2502.01456</a></p><p class="" id="241451cf-7b79-807b-ba92-c0ba196c7ef6">각 토큰 단위로 계산된 log-prob ratio를 사용해 reward를 구성</p><p class="" id="241451cf-7b79-80cc-8a85-df0b09b702ac">근데, reward source는 여전히  <strong>majority voting 기반</strong>이었을 가능성이 큼</p></details></li><p class="" id="241451cf-7b79-80d8-a048-ef56f094a64a">
</p><li><details><summary> <strong>GRPO</strong> : Group Relative Policy Optimization</summary><p class="" id="241451cf-7b79-800c-a5d9-c77d5e1f4c0e"><a href="https://arxiv.org/abs/2402.03300">https://arxiv.org/abs/2402.03300</a></p><p class="" id="240451cf-7b79-80b9-9848-ccfe19cae50b">같은 질문에 대한 여러 응답의 상대적인 정답률을 비교하여 보상을 주는 방식</p><p class="" id="240451cf-7b79-80de-949b-e19359b9af89">N개의 샘플, 다항 reward, 다양성 샘플링, online setting 등에 모두 적용 가능</p></details></li><p class="" id="240451cf-7b79-8044-9555-c8e1fedfeae4">
</p><li><details><summary>DPO : Direct Preference Optimization (사용 안함)</summary><p class="" id="241451cf-7b79-8067-a466-eb328696f023">두 응답 중 어느 쪽이 더 좋은지에 대한 인간의 "선호"를 직접 학습하는 방식</p><p class="" id="241451cf-7b79-8042-a1a7-f565568e4c95">reward가 단순한 0/1 형태의  <strong>pairwise</strong> 비교로 제한</p><p class="" id="241451cf-7b79-8093-9e05-d6527de1708e">preference 들어가서 offline 구조임.</p><p class="" id="241451cf-7b79-803c-b69a-d6961c1154ba"> <strong>→ 이 논문에서 적용 못함.</strong></p></details></li><p class="" id="240451cf-7b79-80c2-949d-c53899472add">
</p><p class="" id="240451cf-7b79-80f7-998b-f4b873c7068f">
</p><h3 class="" id="23f451cf-7b79-809f-8b9b-f422de0e92f7">Figure 6 : Achieves sustainable self-evolution through “online” and “RL”</h3><figure class="image" id="240451cf-7b79-804e-8318-fa45ce179481" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2013.webp" style="width:624px"/></picture></figure><p class="" id="240451cf-7b79-80d8-a0c4-ec28fbf44250">
</p><li><details><summary>pass@1, avg@16, maj@16 이 뭐야?</summary><p class="" id="241451cf-7b79-8041-9087-d36765281472">
</p><p class="" id="240451cf-7b79-80d5-9e68-c2d5cbb7be4c"> <strong>pass@1</strong></p><p class="" id="240451cf-7b79-801e-ad5e-c87c0bc89f13">→ TTRL 모델이 현재 상태로 inference 할 때  <strong>단일 샘플이 정답일 확률</strong></p><p class="" id="240451cf-7b79-80ff-bcfb-c5b73b6da9d3">→ 실제 사용자 성능</p><p class="" id="240451cf-7b79-80e0-aeb5-c99eb32fbf71">
</p><p class="" id="240451cf-7b79-8016-9a03-c130c7b520ca"> <strong>avg@16</strong></p><li style="list-style-type:disc">각 문제에 대해 생성된 16 <strong>개의 답변 중 정답인 비율</strong>을 계산</li><p class="" id="240451cf-7b79-80a5-b2e1-fc91c2973919">→ 예를 들어 정답이 10번 나왔으면 그 문제의 점수는 10/16</p><li style="list-style-type:disc">이렇게 모든 문제에 대해 평균을 낸 것</li><p class="" id="240451cf-7b79-80dd-83a3-ebf319ecbedf">→ ground truth랑 비교해야하니까 사실상 나중에 성능 평가용임</p><p class="" id="240451cf-7b79-8049-96a5-c2b9f47327a5">
</p><p class="" id="240451cf-7b79-80bd-9dcd-c6d2cc501c5d"> <strong>maj@16</strong></p><li style="list-style-type:disc">다수결로 뽑은 수도라벨이  <strong>정답이면</strong> 1점</li><li style="list-style-type:disc">이렇게 전체 문제에 대해 평균을 내면 <code>maj@64</code> accuracy</li><p class="" id="241451cf-7b79-808b-8f6d-d1a7a039432e">
</p></details></li><p class="" id="240451cf-7b79-80e4-81b2-c755fe5e59f8">
</p><p class="" id="240451cf-7b79-80cc-882c-f02141c2f99a">
</p><p class="" id="240451cf-7b79-8030-94a2-cffebf521dc9">TTRL은 단순히 기존의 pseudo-label에 수렴하는 게 아니라,  <strong>pseudo-label 자체도 계속 고도화</strong>되고 있음</p><li style="list-style-type:disc">TTRL의 구조상,  <strong>자신의 예측(y_hat)으로부터 만든 보상을 기반으로 스스로 학습</strong>함</li><li style="list-style-type:disc">그러면 모델의 성능이 향상되고, 그에 따라 더 좋은 예측 → 더 나은 pseudo-label → 더 나은 학습 신호로 이어지는  <strong>자기강화 루프(self-reinforcing loop)</strong> 형성</li><p class="" id="240451cf-7b79-80cb-8c8b-d7e13ee9ebfd">
</p><p class="" id="240451cf-7b79-80b7-801f-e90b6b3270cd"><mark class="highlight-red">이 논문에서는 RL을 쓸 때 maj@16 기준으로 reward를 계산(GRPO) 하고,</mark></p><p class="" id="240451cf-7b79-8040-82fd-d60268e820a6"><mark class="highlight-red">학습 이후 성능 평가에서는 avg@16, maj@16 둘 다 확인함.</mark></p><p class="" id="240451cf-7b79-8092-84eb-f53f81ef3d08">
</p><p class="" id="241451cf-7b79-803c-bf84-ee06159c8099">
</p><p class="" id="241451cf-7b79-809b-be2e-ce6cda8d7881">
</p><p class="" id="241451cf-7b79-80c7-83e9-e65c19eba1f6">
</p><hr id="240451cf-7b79-80bb-8c25-e7e89631b128"/><h1 class="" id="23f451cf-7b79-806c-abcb-fd70797dd70c">4 Analysis and Discussions</h1><p class="" id="23f451cf-7b79-8041-be5b-f85ff54d6b30">
</p><h2 class="" id="23f451cf-7b79-8009-83cc-e5c49554336c">4.1 Q1: <mark class="highlight-red"> <strong>How Well</strong></mark> Can TTRL Perform?</h2><p class="" id="240451cf-7b79-8035-b5a8-f8571b79b529">기존 self-training 방식의 상한선들과 비교해서 어디까지 도달할 수 있는지 실험적으로 검증</p><figure class="image" id="240451cf-7b79-80d2-b2c7-fe2624395036" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2014.webp" style="width:624px"/></picture></figure><p class="" id="240451cf-7b79-8052-b677-f756d195b653">TTRL 전/후의 avg@64, maj@64 비교</p><p class="" id="240451cf-7b79-8037-8e84-de8771482b0b">→ 모든 benchmark에서 TTRL 적용 후 avg@64, maj@64 둘 다 성능이 증가</p><p class="" id="240451cf-7b79-80d4-9ba2-fb794343826b"> <strong>TTRL은 학습 신호로 maj@n을 사용했지만, 학습 이후 결과는 그 상한선을 초과</strong></p><p class="" id="240451cf-7b79-80fa-b6fe-de44eaba5868">
</p><p class="" id="240451cf-7b79-8007-ac7c-efcf01131db1">
</p><figure class="image" id="240451cf-7b79-8052-8752-eb319dde9f85" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2015.webp" style="width:480px"/></picture></figure><p class="" id="240451cf-7b79-8032-b0bb-e6958cb3359f">RL : ground-truth label이 있는 상태에서 RL을 직접 돌리는 경우</p><p class="" id="240451cf-7b79-8048-bd37-ef6d7dbd1660">사실 RL은 Label이 없어서 Test-Time에 불가능 → 그 성능을 따라감<br/><br/></p><p class="" id="240451cf-7b79-8097-97f4-c2f78a0c1893">
</p><li><details><summary>그런데 어떻게 TTRL이 중간 accuracy에서 leakage보다 높아질 수 있나?</summary><p class="" id="240451cf-7b79-80a9-a8e1-c99bebbcc22a">
</p><p class="" id="240451cf-7b79-8067-a823-ccc2b3a70ae0">Leakage RL은 단일 샘플에 대해  <strong>reward가 binary함</strong></p><li style="list-style-type:disc">정답이면 +1, 오답이면 0</li><p class="" id="240451cf-7b79-8028-8438-c7efc8130e8f">→ 이건  <strong>very sparse + very high variance</strong> reward</p><p class="" id="240451cf-7b79-80d1-a79e-c2c2f766f6ed">→ 따라서 초반에는  <strong>policy가 이 reward를 제대로 활용하기 어려움</strong></p><p class="" id="240451cf-7b79-8013-8160-ddf33614e707">
</p><p class="" id="240451cf-7b79-8068-b119-e2477b7bec18">반대로 TTRL은 soft한 avg 기반 reward를 사용함</p><li style="list-style-type:disc">예를 들어 정답이 전체 32개 중 18개면 reward가 0.5625</li><li style="list-style-type:disc">이건  <strong>gradient variance가 낮고 안정적인 학습이 가능</strong></li><p class="" id="240451cf-7b79-8018-9458-e86270a32e3f">
</p><p class="" id="240451cf-7b79-8025-94e3-e54932a28a96">하지만 시간이 지나면, 정답 기반 reward가 더 정확하므로 TTRL보다 더 높은 한계 성능에 수렴하게 됨</p></details></li><p class="" id="240451cf-7b79-8048-b5df-c09f4f31732d">
</p><p class="" id="240451cf-7b79-8096-afde-d782681a5233">
</p><h2 class="" id="23f451cf-7b79-80d6-beaa-d5bc4fc4a318">4.2 Q2: Why Does TTRL Work?</h2><p class="" id="240451cf-7b79-800c-85e1-c246e1f49813">
</p><h3 class="" id="240451cf-7b79-801b-85c5-fe14a346f5d8">1. Label Estimation</h3><figure class="image" id="240451cf-7b79-808b-aa4e-e0f53f1b1958"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2016.webp" style="width:709.9928588867188px"/></picture></figure><p class="" id="240451cf-7b79-809f-8301-ceee99ece220"> <strong>Label Accuracy와 Reward Accuracy는 다르다!!</strong></p><li style="list-style-type:disc">label accuracy는 낮음 (majority voting으로 만든 pseudo-label이 틀리는 경우가 많음)</li><li style="list-style-type:disc">그러나  <strong>reward accuracy는 높게 유지됨</strong></li><figure class="block-color-teal_background callout" id="240451cf-7b79-800e-b5a2-fb7d2f98fbce" style="white-space:pre-wrap;display:flex"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><mark class="highlight-default_background">"</mark><mark class="highlight-default_background"><strong>Lucky Hit</strong></mark><mark class="highlight-default_background">" 현상: sampling된 답 중 정답을 우연히 맞출 수 있고, 이게 높은 보상으로 이어짐</mark></div></figure><p class="" id="240451cf-7b79-8032-95cc-ef963af5d2f3">
</p><p class="" id="240451cf-7b79-8001-9f70-ca10f0c537b2">라벨이 틀릴 수 있어도, reward는 우연히 맞기 때문에 보상 신호는 충분히 유효하고,</p><p class="" id="240451cf-7b79-80e5-97db-f5fed93b9fbd">RL은 원래 그런 noise에 강하므로, label이 부정확해도 학습이 안정적으로 진행될 수 있다.</p><p class="" id="240451cf-7b79-80a3-af18-f9d894565bf9">
</p><p class="" id="240451cf-7b79-808b-8ced-c6589e02f9b4">
</p><h3 class="" id="240451cf-7b79-804e-bdbf-da16b5cf8733">2. Reward Calculations</h3><p class="" id="240451cf-7b79-8044-a970-ea3109738a6c"> <strong>비교 기반이기 때문에 "운 좋게" 올바른 보상을 줄 수 있다</strong></p><li style="list-style-type:disc">ŷ가 label과 같으면 →  <strong>positive reward</strong></li><li style="list-style-type:disc">ŷ가 label과 다르면 →  <strong>negative reward</strong></li><p class="" id="240451cf-7b79-80f9-a59a-d01418bfb746">근데 이 label이 실제 정답이 아닐 수도 있음</p><p class="" id="240451cf-7b79-80ec-8577-d56a3bb2e22d">그래도  <strong>ŷ가 틀린 label이랑도 다르면</strong>, 그건  <strong>"틀린 거다"라는 부정적 신호로는 여전히 맞음</strong></p><p class="" id="240451cf-7b79-80cb-a004-dff6578ca6a4">→ label이 틀려도 reward는 우연히 맞는 경우가 많음</p><figure class="image" id="240451cf-7b79-80a5-b637-cbe59861f08c" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2017.webp" style="width:432px"/></picture></figure><table class="simple-table" id="240451cf-7b79-80bf-8971-f23e8aec9573"><thead class="simple-table-header"><tr id="240451cf-7b79-80ea-ad6e-c74f60cfc924"><th class="simple-table-header-color simple-table-header" id="l=SS" style="width:219px">무엇을 기준으로 reward?</th><th class="simple-table-header-color simple-table-header" id="Y^DN">예측값</th><th class="simple-table-header-color simple-table-header" id="U:ve" style="width:262px">reward</th></tr></thead><tbody><tr id="240451cf-7b79-8000-9960-dcebc6f6b7c8"><td class="" id="l=SS" style="width:219px"> <strong>True label (3)</strong></td><td class="" id="Y^DN"><code>1 1 2 2 2 4 5 6</code></td><td class="" id="U:ve" style="width:262px"><code>0 0 0 0 0 0 0 0</code> → 전부 오답</td></tr><tr id="240451cf-7b79-80b5-8c45-c794199048e6"><td class="" id="l=SS" style="width:219px"> <strong>Estimated label (2)</strong></td><td class="" id="Y^DN"><code>1 1 2 2 2 4 5 6</code></td><td class="" id="U:ve" style="width:262px"><code>0 0 1 1 1 0 0 0</code> → 3개 정답 처리</td></tr></tbody></table><p class="" id="240451cf-7b79-8081-971a-fe1440689a2b">→ 8개 중 5개는 올바른 reward를 줌</p><p class="" id="240451cf-7b79-808f-87da-dc3ae6db093a">
</p><p class="" id="240451cf-7b79-8011-888a-d82ff1ed6eaa"> <strong>rollout 기반 robustness</strong></p><p class="" id="240451cf-7b79-80ed-aaf6-ffe3c1f128fd">하나의 질문에 대해 여러 개 (M개)의 답변을 sampling하기 때문에:</p><li style="list-style-type:disc">하나라도 label과 일치하는 output이 있으면 → positive reward</li><li style="list-style-type:disc">하나도 없더라도 → negative reward는 정확히 계산됨</li><p class="" id="240451cf-7b79-80c5-b913-e49b902155c7">
</p><p class="" id="240451cf-7b79-8057-90df-d8e593a49a28"> <strong>모델이 못할수록 reward accuracy는 오히려 올라간다?</strong></p><p class="" id="240451cf-7b79-8037-851a-c62c5623ef52">AIME 2024에서</p><li style="list-style-type:disc">label accuracy: 37%</li><li style="list-style-type:disc">reward accuracy: 92%</li><p class="" id="240451cf-7b79-805b-a7d5-d64d7879d0cd">모델이 다양한 오답을 내기 때문에 (e.g., 가장 많이 나온 답이 16.6%에 불과)</p><p class="" id="240451cf-7b79-80ac-b2d5-c716e1869159"> <strong>각각의 output이 다 다른 틀린 답</strong>이므로 → label과 일치하지 않음</p><p class="" id="240451cf-7b79-8026-868c-f7825254b303">그 자체로 negative reward가 제대로 전달됨 (비교 결과 다르니까)</p><p class="" id="240451cf-7b79-8014-aa54-f1badd217003">
</p><p class="" id="240451cf-7b79-8099-a119-cf0480545c8e">
</p><h3 class="" id="240451cf-7b79-80dd-9eab-dd6ee52dcc48">3. Online Learning</h3><p class="" id="240451cf-7b79-8030-9ce3-f213a444ad1c">온라인 RL 접근 방식을 기반으로 설계되니까 모델은 application하면서 기능을 향상시킬 수 있으며, 이는 투표를 통해 생성 된보다 정확한 레이블로 이어짐</p><p class="" id="240451cf-7b79-807a-834f-fced1eee9bc3">→ supervision 신호의 품질이 향상되어 지속 가능한 자기 진화가 가능 (Figure 6 내용)</p><p class="" id="240451cf-7b79-806c-ae3f-cef27e6c399a">
</p><p class="" id="240451cf-7b79-8090-92fc-f865b70cc3b7">
</p><p class="" id="23f451cf-7b79-8005-8962-f005e7c8ca82">
</p><h2 class="" id="23f451cf-7b79-80f2-91a5-f5859b178380">4.3 Q3: When Might TTRL Fail?</h2><p class="" id="240451cf-7b79-8034-9e46-c84a6434ac5e">
</p><h3 class="" id="240451cf-7b79-8038-8e9f-fb0a7094500b">Figure 11 : Inappropriate RL Hyperparameters</h3><figure class="image" id="240451cf-7b79-80f5-bb4b-fe5bd3f8cb7c" style="text-align:left"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2018.webp" style="width:384px"/></picture></figure><p class="" id="240451cf-7b79-80cd-b8f7-e2348bee7743">TTRL은  <strong>unsupervised</strong> +  <strong>reward estimation이 noisy</strong>한 구조이기 때문에,</p><p class="" id="240451cf-7b79-80c0-8e39-e64a7e99bc20">일반적인 RL보다  <strong>하이퍼파라미터에 훨씬 민감</strong>함.</p><li style="list-style-type:disc">특히 실패한 경우는  <strong>Entropy</strong>가 끝까지 낮아지지 않음 (→ exploration 실패)</li><li style="list-style-type:disc">실험적으로 다음 두 가지가 핵심임:</li><p class="" id="240451cf-7b79-80b8-95c3-c2dda585cbeb">
</p><p class="" id="240451cf-7b79-807b-bc23-e97756013e40"> <strong>(1) Temperature</strong></p><li style="list-style-type:disc"><code>T=1.0</code>으로 높이면 더 많은 entropy (더 다양한 답변)</li><li style="list-style-type:disc">exploration이 많아지고 prior knowledge를 더 잘 쓰게 됨</li><li style="list-style-type:disc">challenging benchmark (ex. AIME)에선 exploration이 매우 중요</li><p class="" id="240451cf-7b79-808b-9b4e-f014d904f0eb"> <strong>(2) Episodes</strong></p><li style="list-style-type:disc">문제 수 적고 난이도 높은 데이터셋 (ex. AIME 2024)은  <strong>에피소드 수가 많아야 함</strong></li><li style="list-style-type:disc">exploration을 충분히 하지 않으면 수렴 불가</li><p class="" id="240451cf-7b79-80dd-b969-fd7b3bf3c87a">
</p><p class="" id="240451cf-7b79-8071-b8c4-c39d1b41ffef">→ TTRL은 문제의 난이도/분포/규모에 따라 하이퍼파라미터를  <strong>정밀 조정해야 함 </strong></p><p class="" id="240451cf-7b79-80bc-b338-c36125e4fcd6">
</p><p class="" id="240451cf-7b79-80c5-8392-fa3c9882e2a6">
</p><h3 class="" id="240451cf-7b79-80c2-bcfc-e550e9f14c73">Table 3: Lack of Prior Knowledge on Target Task</h3><figure class="image" id="240451cf-7b79-805e-adb4-e9f1980b0a2f"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2019.webp" style="width:709.9921875px"/></picture></figure><p class="" id="240451cf-7b79-8051-aa0b-fab15b1dfb93">TTRL은  <strong>test set만 가지고 학습</strong>하기 때문에,</p><p class="" id="240451cf-7b79-80f2-9b8f-ec9f795cceed">모델이 그 분야에 대한 사전 지식이 없으면  <strong>완전히 실패할 수 있음</strong></p><li style="list-style-type:disc">curriculum learning (쉬운 문제부터) 같은 도입이 없음</li><li style="list-style-type:disc"> <strong>사전 학습된 knowledge 없이 어려운 문제에 바로 적응</strong>해야 함</li><p class="" id="240451cf-7b79-80da-8929-cc1ef3ea5746">
</p><li style="list-style-type:disc">난이도가 높아질수록  <strong>성능 향상 폭 감소</strong></li><li style="list-style-type:disc">문제 길이 감소율도 떨어짐</li><p class="" id="240451cf-7b79-8091-839b-d28dfa4aaaef">→ 어려운 문제일수록 backbone의 사전 지식 부족으로 학습이 힘들다는 증거</p><p class="" id="241451cf-7b79-80c0-a4de-fd17f59e6aa9">
</p><p class="" id="240451cf-7b79-8035-8cca-e6868067d2d7">
</p><hr id="240451cf-7b79-80a6-aabe-cacd912d5aa6"/><h1 class="" id="23f451cf-7b79-80b8-a3cc-c24193d30661">7 Limitations and Future Works</h1><h3 class="" id="240451cf-7b79-805f-85a2-f6f3ac8d53c8">Limitations</h3><ol class="numbered-list" id="240451cf-7b79-8039-8a2a-d7a71416c918" start="1" type="1"><li> <strong>TTRL은 초기 탐색 단계</strong>에 불과하며,</li></ol><ol class="numbered-list" id="240451cf-7b79-806a-80fe-d687c0c5ae5a" start="2" type="1"><li>다음 두 요소가  <strong>학습 성능에 큰 영향을 미침</strong>에도 아직 정량적 분석이 부족함:<li style="list-style-type:disc">모델의  <strong>사전 지식 수준 (prior knowledge)</strong></li><li style="list-style-type:disc"> <strong>하이퍼파라미터 설정</strong> (temperature, episode 수 등)</li></li></ol><p class="" id="240451cf-7b79-802b-b0f0-eee1ad9928fa">
</p><h3 class="" id="240451cf-7b79-8004-8de6-f14b2c1527b9">Future Works (논문에 나온 내용)</h3><ol class="numbered-list" id="240451cf-7b79-80a7-81bb-d0c3c889a381" start="1" type="1"><li> <strong>이론적 분석</strong><li style="list-style-type:disc">TTRL이 4.1에서 정의한  <strong>두 upper bound</strong>(maj@n / RL leakage)에 대해 얼마나 수렴 가능한지 이론적으로 분석</li><li style="list-style-type:disc">convergence theory와 optimality 조건 규명</li></li></ol><ol class="numbered-list" id="240451cf-7b79-80b9-86ba-d18380311334" start="2" type="1"><li> <strong>스트리밍 데이터 기반 온라인 학습</strong><li style="list-style-type:disc">현재 TTRL은 static test set 기준</li><li style="list-style-type:disc">이를  <strong>실시간 도착하는 데이터 스트림에 적응하는 형태로 확장</strong>하려는 계획<p class="" id="240451cf-7b79-8067-b50f-fb7b323a1ff9">→ 진정한 Test-Time Adaptation (TTA)으로의 확장</p></li></li></ol><ol class="numbered-list" id="240451cf-7b79-803d-9e3e-c2732f68382e" start="3" type="1"><li> <strong>대규모 self-supervised RL</strong><li style="list-style-type:disc">TTRL을  <strong>대규모 데이터셋 + 대형 LLM</strong>에 적용</li><li style="list-style-type:disc">인간의 라벨링 없이도 강력한 자기지도 강화학습 시스템으로 발전시키려는 방향</li></li></ol><ol class="numbered-list" id="240451cf-7b79-80bd-b55b-fad116fa1c81" start="4" type="1"><li> <strong>Agentic Task 및 과학적 추론</strong><li style="list-style-type:disc">TTRL을 단순 QA 또는 math benchmark가 아닌,<li style="list-style-type:circle"> <strong>장기적 계획이 필요한 agentic task</strong></li><li style="list-style-type:circle"> <strong>여러 단계의 논리를 요하는 과학적 문제 해결</strong>에 확장</li></li><li style="list-style-type:disc">open-ended한 domain으로도 TTRL 적용 가능성 타진</li></li></ol><p class="" id="241451cf-7b79-8090-8ec5-c11fb855b2a9">
</p><h3 class="" id="241451cf-7b79-80d5-9027-e52aeb637ba5">Limitations &amp; Future Works (내 생각)</h3><p class="" id="241451cf-7b79-80cc-a9a6-d92a0bfdac82">
</p><ol class="numbered-list" id="241451cf-7b79-804b-9a87-ee40c061c430" start="1" type="1"><li>Hyperparameter Sensitivity<p class="" id="241451cf-7b79-80c8-bad5-c10a66e0c334">RL training is  <strong>highly sensitive</strong> to hyperparameters.</p><p class="" id="241451cf-7b79-805b-ba7f-d3769873d45e">→Automatic hyperparameter tuning</p><p class="" id="241451cf-7b79-80e1-8150-dbaae1ef4072">
</p></li></ol><ol class="numbered-list" id="241451cf-7b79-8018-a37b-e9825a07c111" start="2" type="1"><li>Too much resource<p class="" id="241451cf-7b79-8067-88bb-d97cf8058236">The experiments require  <strong>8 × A100 80GB GPUs</strong></p><p class="" id="241451cf-7b79-8035-8f5f-db22d99f009c">→ Parameter-efficient by LoRA</p><p class="" id="241451cf-7b79-8005-aa36-c5c196c40c59">
</p></li></ol><ol class="numbered-list" id="241451cf-7b79-80dd-b286-e1b1b14a2673" start="3" type="1"><li>Only for simple QA<p class="" id="241451cf-7b79-8064-9aca-d53b7801ea52">Experiments are focused on  <strong>math &amp; multiple-choice</strong></p><p class="" id="241451cf-7b79-8043-87ed-c5e2e99c9ebf">→ Extend to complex, multi-step reasoning tasks</p></li></ol><p class="" id="241451cf-7b79-800d-b66a-c005eddd519d">
</p><p class="" id="241451cf-7b79-801c-bbed-efdd81f3e4b9">
</p><p class="" id="241451cf-7b79-8085-bc7c-c827ce24ece9">
</p><hr id="240451cf-7b79-8084-9117-e75ba7f23fc5"/><h1 class="" id="241451cf-7b79-8098-aac3-fe8fc55f4004">Q&amp;A</h1><p class="" id="241451cf-7b79-804e-a08f-de2e99a03afc">논문 Presentation 발표 중 제대로 답변 못한 Q&amp;A</p><p class="" id="241451cf-7b79-8091-b5e2-fdd21f930bc5">
</p><h3 class="block-color-orange_background" id="241451cf-7b79-80b8-9270-d2fbb339a555">Q1) 이거 Test-Time에 RL 첫 논문 맞는가?</h3><p class="" id="241451cf-7b79-80c5-b9fd-d2243371647f">이론상 Test-Time + RL 구조의 첫 논문은 아니다.</p><p class="" id="241451cf-7b79-80c5-a40b-c80d1c8b284f">
</p><p class="" id="241451cf-7b79-8091-a8f0-fffe5a3654ea">GRPO 자체도 test-time에 쓸 수 있고, label 없이도 reward 만들 수 있지 않나?</p><li style="list-style-type:disc">GRPO는  <strong>RL 알고리즘 (optimizer)</strong></li><li style="list-style-type:disc">TTRL은  <strong>전체 프레임워크</strong></li><p class="" id="241451cf-7b79-803f-827d-db4e4272947d">
</p><p class="" id="241451cf-7b79-80b3-bd50-e650eb983562"><a href="https://arxiv.org/abs/2402.03300">https://arxiv.org/abs/2402.03300</a></p><p class="" id="241451cf-7b79-8090-b7f3-ec2c29fa2bb9">GRPO를 소개한 DeepSeekMath</p><p class="" id="241451cf-7b79-809c-a591-d60151a943c4">→ GRPO를 test-time에서 label 없이 쓰는 건 가능</p><p class="" id="241451cf-7b79-80fc-967c-cd37744347c2">
</p><p class="" id="241451cf-7b79-807c-84c9-c5fd04346c1c"> <strong>TTRL에서 정의한 GRPO를 test-time에 실제로 쓰려면 필요한 요소들</strong></p><li style="list-style-type:disc">test-time input stream 처리 방식</li><li style="list-style-type:disc">sampling strategy (M개 생성 → voting)</li><li style="list-style-type:disc">pseudo-label → reward 변환 함수</li><li style="list-style-type:disc">GRPO를 작동시킬 수 있는 reward scaling</li><li style="list-style-type:disc">batch-level update → continual self-evolution</li><p class="" id="241451cf-7b79-802d-8b16-f9c5afc8b7ff">→ 실제로 작동하게 만드는 <mark class="highlight-red"> <strong>환경 + 입력 + reward + 반복 학습 루프</strong></mark>를 처음 설계</p><p class="" id="241451cf-7b79-806d-801a-c29fece2f491">
</p><p class="" id="241451cf-7b79-801e-8c05-da87cd331180">
</p><p class="" id="241451cf-7b79-8028-a3c1-f7136e982b00"><a href="https://arxiv.org/abs/2505.18514">https://arxiv.org/abs/2505.18514</a></p><p class="" id="241451cf-7b79-80f8-8f32-e624966c9533">우리 연구실 지도교수님이신 <a href="https://taesikgong.com/">공태식 교수님</a>이 최근에 쓰신 이 논문도 “Test-Time RL”의 정의가 맞음</p><p class="" id="241451cf-7b79-8089-ac56-d14a5699dee3">→ Test-Time RL의 최초라는 말은 틀림.</p><p class="" id="241451cf-7b79-80d4-8ec1-e61daf57f592">차이점은</p><p class="" id="241451cf-7b79-8088-9426-f11411cac7aa">BiTTA : 정답 클래스 자체는 필요하지 않지만, 실시간으로 사람의  <strong>Binary Feedback</strong>이 필요하다.</p><p class="" id="241451cf-7b79-809a-9598-c39e36008567">TTRL : 진짜로  <strong>label-free</strong>,  <strong>oracle-free</strong></p><p class="" id="241451cf-7b79-808b-8be5-e7151542fe45">
</p><p class="" id="241451cf-7b79-80eb-848a-fc508dce2a99">정답이 있는 oracle의 binary feedback을 필요로 함.</p><p class="" id="241451cf-7b79-80ca-9073-f8846d63903f">
</p><p class="" id="241451cf-7b79-8022-b3e4-c89180ba99ad">TTRL은 어떠한 모델, 데이터셋에도 적용 가능하지만, (정답이 딱 떨어지기만 하면)</p><p class="" id="241451cf-7b79-8057-8187-ec923329f8c7">reward noise, model prior, dataset 난이도 등의 영향으로 </p><p class="" id="241451cf-7b79-80ca-98fc-fc4f81811ca1">hyperparameter (batch size, temperature, episode 수 등)에 매우 민감</p><p class="" id="241451cf-7b79-80f1-99a5-f1a7eef886cc">→ 하이퍼파라미터 튜닝이 필요</p><p class="" id="241451cf-7b79-80b6-8fa2-ed496667e351">
</p><p class="" id="241451cf-7b79-80cd-b24f-c7876c009550">
</p><h3 class="block-color-orange_background" id="241451cf-7b79-8072-a18d-dfd8cb8ce57a">Q2) Lucky hit여도 결국 틀린답으로 학습하는거 아닌가?</h3><p class="" id="241451cf-7b79-809b-99b5-c43fbff990eb">완전 잘못 이해하고 있었다. 내가 알고있던 정보는 BiTTA 처럼 Reward를 1과 -1로 줘야지 가능한 것이다.</p><p class="" id="241451cf-7b79-804d-bc07-cfd2fee1e456"><a href="https://arxiv.org/abs/2505.18514">https://arxiv.org/abs/2505.18514</a></p><p class="" id="241451cf-7b79-8045-a2c8-f67bb19dd09b">예측이 틀렸으면  <strong>-1이라는 부정적인 보상</strong>을 명시적으로 줘서, 틀린 방향으로의 확률이 낮아지도록 gradient</p><p class="" id="241451cf-7b79-8043-a679-cbfcce9d7a3d">
</p><p class="" id="241451cf-7b79-80e1-8127-f72ff6f1dad6">
</p><figure class="image" id="241451cf-7b79-8051-b184-d38434d71949"><picture><img class="img-fluid rounded z-depth-1" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" src="/files/2025-07-31-ttrl/image%2020.webp" style="width:336px"/></picture></figure><p class="" id="241451cf-7b79-803f-ba7b-d6ff6448f496">
</p><p class="" id="241451cf-7b79-80cc-a3d7-d568a602a7f3">예시 - 모델이 잘 모를때</p><p class="" id="241451cf-7b79-80a9-9749-dcf54a22a4b9"> <strong>[1, 1, 2, 2, 2, 4, 5, 6]</strong> → majority는 2 (2개)</p><p class="" id="241451cf-7b79-80f9-92d0-ccc4438b1721">→ 3/8이라서 적긴 하지만,  <strong>분명히 2가 정답이라는 것이 강화되는 건 맞다.</strong></p><p class="" id="241451cf-7b79-8062-a4d1-f00522aa4b4e">
</p><p class="" id="241451cf-7b79-80a0-99d5-e287ca328da3">TTRL에서 모델이 잘 몰라서 majority voting을 통해 뽑은 pseudo label = 2</p><p class="" id="241451cf-7b79-8031-a315-f404633c81b8"><code>0 0 1 1 1 0 0 0 </code></p><p class="" id="241451cf-7b79-8025-9709-c333209f3f4b">신호로 Reinforcement Learning</p><p class="" id="241451cf-7b79-800e-9e69-e646ac5d37b5">
</p><p class="" id="241451cf-7b79-8021-9e56-cf1d89c09dc4">만약 실제로 true-label (3)을 줄 때</p><p class="" id="241451cf-7b79-808a-adad-d7e253a1a401"><code>0 0 0 0 0 0 0 0</code></p><p class="" id="241451cf-7b79-80aa-99f8-c5940cb64edb">신호로 Reinforcement Learning</p><p class="" id="241451cf-7b79-80f5-a473-f82d58458365">
</p><p class="" id="241451cf-7b79-8072-85da-d3200f911a0a"> <strong>모델이 잘 모르는 것에 대해서 label이 없이 했지만, </strong></p><p class="" id="241451cf-7b79-8022-a736-fee0930b2f83"> <strong>실제 정답 label이 있을때와 Reward 신호가 62.5%나 일치한다! → hit ratio</strong></p><p class="" id="241451cf-7b79-80ff-8214-c8659ef05cb5">
</p><p class="" id="241451cf-7b79-80e7-a6d4-e53b0c3cb0ec">실제 정답 라벨이 없기 때문에 다른 논문처럼 reward에서 penalty 신호인 -1을 주지 않고,</p><p class="" id="241451cf-7b79-8092-b661-e31aede932ae">맞으면 1,  <strong>틀려도 0 </strong>으로 설정한 것으로 보인다.</p><p class="" id="241451cf-7b79-8050-92fb-c08291555057">
</p><p class="" id="241451cf-7b79-80b7-a6f1-fad5b001a4dd">
</p><p class="" id="241451cf-7b79-800c-baed-cb146828c330">
</p><h3 class="block-color-orange_background" id="241451cf-7b79-801a-9d07-c0c5281a3c1c">Q3) 이 RL에서 action이 뭔가?</h3><p class="" id="241451cf-7b79-802b-b5e4-f21ff7617291">state : 주어진 문제(prompt) x</p><p class="" id="241451cf-7b79-80f2-a391-fda4552ab895">action : LLM의 답변 행위</p><p class="" id="241451cf-7b79-8050-8625-e83472cdd0a5">→ LLM은 그에 대한 답변 y를 policy <strong> </strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span class="notion-text-equation-token" contenteditable="false" data-token-index="0" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{\theta}(y \mid x)
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span>로부터 생성 (sampling)</p><p class="" id="241451cf-7b79-80cf-9bde-c673c347a13b">
</p><p class="" id="241451cf-7b79-8076-af9d-c91db6bffced">
</p><p class="" id="241451cf-7b79-8031-ad3a-fa42fbaca653">
</p><span class="sans" style="font-size:14px;padding-top:2em"></span>