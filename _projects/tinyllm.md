---
layout: pagein
title: Tiny LLM
description: Explored LLM architectures optimized for resource-constrained environments.
img: assets/img/projects/tinyllm.webp
importance: 1
category: Research
github: https://github.com/unist-uai/TinyLLM
github_stars: false
related_publications: false
giscus_comments: true
pretty_table: true

toc:
  beginning: false  # ë§¨ ì•ì— ëª©ì°¨
  sidebar: false
---

<a href="https://github.com/unist-uai/TinyLLM" rel="external nofollow noopener" target="_blank">
  <img class="only-light" alt="unist-uai/TinyLLM" src="https://github-readme-stats.vercel.app/api/pin/?username=unist-uai&amp;repo=TinyLLM&amp;theme=default&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2">
  <img class="only-dark" alt="unist-uai/TinyLLM" src="https://github-readme-stats.vercel.app/api/pin/?username=unist-uai&amp;repo=TinyLLM&amp;theme=dark&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2">
</a>

<!-- ---

<div style="position: relative; width: 100%; padding-top: 100%; overflow: hidden;">
  <iframe 
    src="https://drive.google.com/file/d/12tSDaqOQBxmZuvWdnXFyJrRk77va2EH5/preview"
    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;"
    frameborder="0"
    allowfullscreen
  ></iframe>
</div> -->

---

### ğŸ’¬ Thoughts

This was the very first project I worked on after joining a research lab as an intern â€” and honestly, it felt like a proper first project. On my first day, I was handed a Jetson Nano and told to set it up. My first thought was: what is this? It's so slow and frustrating! That was basically my first real encounter with Ubuntu. It felt like dealing with an old computer, but surprisingly it supported CUDA (though not the latest versions).

At the time, I had no idea what I was even doing. Dataset? HuggingFace? What are those? I only started to get it after seeing others measuring accuracy â€” oh, so this is on-device AI! It's running entirely on this GPU without any server. Thatâ€™s when it clicked. When I saw models getting only 30% accuracy, I was like, is this even working? Feels like random guessing. Turns out it was because no fine-tuning had been done yet.

It was also fascinating to learn that there are so many tiny LLMs out there, and the goal was to compare them in terms of latency and accuracy to see which ones perform best.

I also got introduced to WandB and started learning how to write automation scripts. It was actually my first time automating anything. Since the measurements were slow and involved lots of repetition, I finally understood why automation matters. Ever since then, Iâ€™ve preferred automating workflows in all my projects.

Looking back, I had no idea what I was doing and just followed along at first â€” but I think my professor intentionally gave me this as a starting point: explore whether LLMs could run on small edge devices and what kind of efficiency they could reach. But along the way, I ended up learning so many valuable things â€” Docker, WandB, automation, conda environments, data visualization, and more.

It might not seem like a big deal later, but for someone who started out knowing nothing, this was a really important project. It gave me a solid foundation and made me feel like I was finally part of a research group.

---

### ğŸ’¬ ëŠë‚€ ì 

ì²˜ìŒìœ¼ë¡œ ì—°êµ¬ì‹¤ ì¸í„´ì„ ì‹œì‘í•˜ë©´ì„œ ì§„í–‰í•œ í”„ë¡œì íŠ¸ë‹¤. ì‚¬ì‹¤ìƒ ì²« í”„ë¡œì íŠ¸ ë‹¤ìš´ í”„ë¡œì íŠ¸ì¸ ê²ƒ ê°™ë‹¤. ë“¤ì–´ê°€ìë§ˆì Jetson Nanoë¥¼ ì£¼ì…”ì„œ ì„¸íŒ…ì„ í•´ë³´ë©´ì„œ ì™€ ì´ê²Œ ë­ì§€? ì™„ì „ ëŠë¦¬ê³  ë‹µë‹µí•˜ë‹¤! í•˜ë©´ì„œ ì‹¤ì œ ìš°ë¶„íˆ¬ë¥¼ ì‚¬ì‹¤ìƒ ì²˜ìŒ ë§Œì ¸ë´¤ë‹¤. ì˜›ë‚  ì»´í“¨í„° ë§Œì§€ë“¯ì´ ë‹µë‹µí–ˆëŠ”ë° GPUëŠ” ë˜ CUDAê°€ ì§€ì›ì´ ë˜ë„¤? (ë¬¼ë¡  ìµœì‹ ë²„ì „ì€ ì•ˆë¨)

ì²˜ìŒì— ì´ê²Œ ë­í•˜ëŠ”ê±´ì§€ë„ ëª°ëëŠ”ë°, ë°ì´í„°ì…‹? í—ˆê¹…í˜ì´ìŠ¤? ì´ê²Œ ë­ì§€? í•˜ë‹¤ê°€ ë‹¤ë¥¸ ë¶„ë“¤ì´ ì •ë‹µë¥  ì²´í¬í•˜ëŠ”ê±° ë³´ê³  ì•„! ì´ê²Œ ì˜¨ë””ë°”ì´ìŠ¤ AIêµ¬ë‚˜! ì„œë²„ ì—†ì´ ì´ GPUë¡œ ëŒë¦¬ëŠ”ê±°êµ¬ë‚˜! ê¹¨ë‹¬ì•˜ë‹¤. ê·¼ë° ì •ë‹µë¥  30% ë‚˜ì˜¤ëŠ”ê±°ë³´ê³  "ì´ê²Œ ë§ë‚˜? ì°ëŠ”ê±°ë‘ ë˜‘ê°™ì€ë°?" í–ˆëŠ”ë° ê·¸ëƒ¥ finetunning ì•ˆí•˜ê³  ëŒë ¤ì„œ ê·¸ëŸ° ê²ƒ ê°™ë‹¤. Tinyí•œ LLMë“¤ì´ ìƒê°ë³´ë‹¤ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆê³ , ê·¸ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ Latencyì™€ ì •í™•ë„ê°€ ê´œì°®ì€ì§€ ì²´í¬í•´ë³´ëŠ”ê²Œ ì‹ ê¸°í–ˆë‹¤.

ê·¸ëŸ¬ë©´ì„œ MLOpsì¸ WandBë„ ì•Œê²Œ ë˜ì—ˆê³ , ìë™í™” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“¤ì–´ì„œ í•˜ëŠ” ë°©ë²•ë„ ì•Œê²Œ ë˜ì—ˆë‹¤. ì‚¬ì‹¤ ìë™í™”ëŠ” ì²˜ìŒ ì¨ë´¤ë‹¤. ì¸¡ì •ì´ ì˜¤ë˜ê±¸ë¦¬ê³  ê°™ì€ ì‘ì—…ì„ ë°˜ë³µí•˜ë‹¤ë³´ë‹ˆ ìë™í™”ë¥¼ ì™œ ì“°ëŠ”ì§€ ì´í•´ê°€ ë˜ì—ˆê³ , ì´ ì´í›„ í”„ë¡œì íŠ¸ì—ì„œë„ ìë™í™”ë¥¼ ì„ í˜¸í•˜ê²Œ ë˜ì—ˆë‹¤.

ì–¼ë ëš±ë•… ì²˜ìŒì— ëª°ë¼ì„œ ë”°ë¼ê°€ê¸°ë§Œ í–ˆì§€ë§Œ, ê²°êµ­ ì‘ì€ On-deviceì—ì„œë„ ëŒë¦¬ê¸° ìœ„í•œ LLMì´ ìˆëŠ”ì§€, ì–´ëŠì •ë„ì˜ íš¨ìœ¨ì´ ë‚˜ì˜¤ëŠ”ì§€ ì•Œì•„ë³´ë¼ê³  êµìˆ˜ë‹˜ì´ ì²« ì‹œì‘ì„ ë˜ì ¸ì¤€ ê²ƒ ê°™ë‹¤. ê·¼ë° ìƒê°ë³´ë‹¤ Docker, WandB, ìë™í™”, conda, í™˜ê²½ì„¸íŒ…, ë°ì´í„° ì‹œê°í™” ë“±ë“±... ë‹¤ë¥¸ ë¶€ë¶„ì—ì„œë„ ì—„ì²­ë‚œ ë„ì›€ì´ ë˜ì—ˆë‹¤. ë‚˜ì¤‘ê°€ë©´ ë³„ê±° ì•„ë‹Œê²ƒ ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ì•„ë¬´ê²ƒë„ ëª°ëë˜ ë‚˜ì—ê²Œ ë§ì€ ì •ë³´ë“¤ì„ ì•Œê²Œí•´ì¤€ ì¢‹ì€ ì—°êµ¬ì‹¤ì˜ ì‹œì‘ì§€ì ì´ë¼ì„œ ê¸°ì–µì— ë‚¨ëŠ” ê²ƒ ê°™ë‹¤.