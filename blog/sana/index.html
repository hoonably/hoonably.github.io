<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers | hoonably </title> <meta name="author" content="Jeonghoon Park"> <meta name="description" content="Undergraduate student at UNIST (CSE), interested in AI, optimization, and systems programming. "> <meta name="keywords" content="UNIST, computer science, undergraduate, on-device AI, optimization, problem solving"> <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"> <meta http-equiv="Pragma" content="no-cache"> <meta http-equiv="Expires" content="0"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/profile/profile.webp?b5bec9d555338d1029578e54703400f7"> <link rel="stylesheet" href="/assets/css/pretendard-subset.css"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hoonably.github.io/blog/sana/"> <script src="/assets/js/theme.js?c06f90403cefcf6ac822b3d318259fcc"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-dark.css?7f7b80282ea53f5dd6b976ff3d9857f9" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> hoonably </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="pagein"> <div class="post"> <header class="post-header"> <h1 class="post-title">SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers</h1> <p class="post-meta"> Created on May 12, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> Paper</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Authors: Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, Song Han Venue &amp; Year: 25, ICLR, Oral 날짜: 2025년 3월 15일</p> <table> <thead> <tr> <th>ArXiv</th> <th><a href="https://arxiv.org/abs/2410.10629" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2410.10629</a></th> </tr> </thead> <tbody> <tr> <td>Project Page</td> <td><a href="https://nvlabs.github.io/Sana/" rel="external nofollow noopener" target="_blank">https://nvlabs.github.io/Sana/</a></td> </tr> <tr> <td>Github Code</td> <td><a href="https://github.com/NVlabs/Sana" rel="external nofollow noopener" target="_blank">https://github.com/NVlabs/Sana</a></td> </tr> </tbody> </table> <blockquote class="block-warning"> <p>💡</p> <p><strong>Key Differentiator</strong></p> <ol> <li> <p>Efficient Linear DiT design</p> <p>ReLU 기반 Linear Attention 도입</p> <p>Mix-FFN Block</p> </li> <li> <p>Deep Compression Autoencoder</p> <p>→ 이로 인한 32배 압축 가능으로 연산도 빨라짐</p> </li> </ol> </blockquote> <figure> <picture> <img src="/files/2025-05-12-sana/image.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>이번에는 SVDQuant의 저자인 Song Han이 또 일을 냈다.</p> <p>SANA 라는 Diffusion 모델을 NVIDIA에서 제작했는데, 역대급이다.</p> <p>내가 하려던 On-device 4K Diffusion 연구에도 크게 도움될 것 같아서 읽어보았다.</p> <h1 id="1-introduction">1. Introduction</h1> <p>지난 1년동안 Diffusion 모델은 text-to-image 연구에서 상당한 진전을 보임.</p> <p>하지만, 아래와 같이 상업 모델은 파라미터가 매우 커짐 → 높은 학습 및 추론 비용을 초래하여 비용이 많이 들음.</p> <blockquote> <p>Industry models are becoming increasingly large, with parameter counts escalating from PixArt’s 0.6B parameters to SD3 at 8B, LiDiT at 10B, Flux at 12B, and Playground v3 at 24B.</p> </blockquote> <p>cloud 뿐만 아니라 edge devices에서도 빠르게 실행되는 고해상도 image generator를 개발할 수 없을까?</p> <figure> <picture> <img src="/files/2025-05-12-sana/image%201.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>이 논문은 1024 × 1024 ~ 4096 × 4096 범위의 해상도에서 이미지를 효율적이고 비용 효율적으로 훈련하고 합성하도록 설계된 파이프 라인 인 SANA를 제안</p> <p>Pixart-σ (Chen et al., 2024a)를 제외하고는 4K 해상도 이미지 생성을 직접 탐색하지 못했습니다. 그러나 Pixart-σ는 4K 해상도에 가까운 이미지를 생성하는 것으로 제한되며 (3840 × 2160) 이러한 고해상도 이미지를 생성 할 때 비교적 느립니다. 이 야심 찬 목표를 달성하기 위해 몇 가지 핵심 디자인을 제안합니다.</p> <h1 id="2-methods">2. METHODS</h1> <h2 id="21-deep-compression-autoencoder">2.1 DEEP COMPRESSION AUTOENCODER</h2> <h3 id="211-preliminary">2.1.1 PRELIMINARY</h3> <p>원래 diffusion 모델은 이미지 픽셀 공간 (pixel space) 위에서 직접 작동 → 훈련, 추론 둘다 너무 느리고 무거움</p> <p><strong>Latent Diffusion Models</strong></p> <p>Autoencoder로 이미지 압축 후 압축된 latent 공간 위에서 diffusion을 돌리자!</p> <p>→ 8배 압축 사용</p> <ul> <li>Pixel space: \(\mathbb{R}^{H \times W \times 3}\)</li> <li>Latent space: \(\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C}\)<br> 여기서 \(C\)는 latent 채널 수</li> </ul> <p><strong>Diffusion Transformer (DiT)</strong></p> <p>추가로 latent feature를 Patch 단위로 또 나눠서 처리</p> <p>패치크기가 PxP 라면 최종적으로 다루는 토큰 개수는</p> \[\frac{H}{PF} \times \frac{W}{PF}\] <p>기존 latent diffusion 모델들(PixArt, SD3, Flux 등)은 보통 다음 세팅을 씀</p> <ul> <li> <strong>AE-F8C4P2</strong> 또는 <strong>AE-F8C16P2</strong> <ul> <li> <strong>F8</strong>: Autoencoder가 8배 압축</li> <li> <strong>C4</strong> 또는 <strong>C16</strong>: latent 채널 수 (4개나 16개)</li> <li> <strong>P2</strong>: Patch 크기 2×2로 묶기</li> </ul> </li> </ul> <p>기존처럼 8배 압축만 하면 계산량이 여전히 너무 많음</p> <p>그래서 SANA는 과감하게 <strong>32배 압축(AE-F32)</strong>하고, 패치로는 묶지 않음.</p> <h3 id="212-autoencoder-design-philosophy">2.1.2 AUTOENCODER DESIGN PHILOSOPHY</h3> <table> <thead> <tr> <th>구분</th> <th>기존 (PixArt, Flux)</th> <th>SANA</th> </tr> </thead> <tbody> <tr> <td>AE 압축비</td> <td>8배 (F=8)</td> <td>32배 (F=32)</td> </tr> <tr> <td>Patchify (P=2)</td> <td>O (패치로 묶음)</td> <td>✖️ (패치 안 묶음)</td> </tr> <tr> <td>최종 Token 수</td> <td>줄였지만 아직 많음</td> <td>훨씬 적음 (16배 감소)</td> </tr> </tbody> </table> <figure> <picture> <img src="/files/2025-05-12-sana/image%204.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>위의 표를 보면 알 수 있듯이 32배 압축하더라도 점수가 크게 떨어지지 않는 모습을 보임</p> <h3 id="213-ablation-of-autoencoder-designs">2.1.3 ABLATION OF AUTOENCODER DESIGNS</h3> <ul> <li><strong>어디서 압축을 더 하는 게 좋은가? (AE vs DiT)</strong></li> </ul> <figure> <picture> <img src="/files/2025-05-12-sana/image%205.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <table> <thead> <tr> <th>설정</th> <th>설명</th> </tr> </thead> <tbody> <tr> <td>AE-F8C16P4</td> <td>8배 압축 + 패치 크기 4</td> </tr> <tr> <td>AE-F16C32P2</td> <td>16배 압축 + 패치 크기 2</td> </tr> <tr> <td>AE-F32C32P1</td> <td>32배 압축 + 패치 사용하지 않음 (SANA)</td> </tr> </tbody> </table> <p>AE-F32C32P1 설정이 가장 뛰어난 성능(FID, CLIP Score)을 기록</p> <p>Autoencoder가 압축을 전적으로 담당하는 것이 성능 및 훈련 안정성 모두에서 가장 우수</p> <ul> <li><strong>Autoencoder latent 채널 수를 몇 개로 하는 게 좋은가?</strong></li> </ul> <figure> <picture> <img src="/files/2025-05-12-sana/image%206.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>C=16, C=32, C=64 실험 수행</li> <li>C=16은 정보 손실로 인해 품질 저하 발생</li> <li>C=64는 복원 품질은 좋았으나 모델 복잡도가 급격히 증가하여 비효율적임</li> <li>C=32가 성능과 효율 사이에서 최적 균형을 달성함</li> </ul> <h2 id="22-efficient-linear-dit-design">2.2 EFFICIENT LINEAR DIT DESIGN</h2> <ul> <li>기존 diffusion transformer(예: DiT) 구조는 <strong>Self-Attention</strong>을 사용함.</li> <li>Self-Attention의 연산량은 <strong>O(N²)</strong> 에 비례함. <ul> <li>NNN은 입력 토큰 수</li> <li>토큰 수가 많아지면 연산량이 급격히 커짐</li> </ul> </li> <li>4K 해상도 이미지를 다루려면, latent token 수가 많아질 수밖에 없음.</li> </ul> <p>→ 이때, 기존 연구들은 이 문제를 해결하려고 해상도 낮추거나 Token 수를 줄였음.</p> <h3 id="relu-기반-linear-attention-도입">ReLU 기반 Linear Attention 도입</h3> <p>기존 Softmax 기반 Attention을 제거하고, <strong>ReLU를 이용한 Linear Attention</strong>을 채택</p> \[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(QK^\top) V\] <p>Softmax는 모든 Query-Token 조합을 다 계산하기 때문에 O(N²) 복잡도가 발생</p> <figure> <picture> <img src="/files/2025-05-12-sana/image%208.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>SANA에서는 다음처럼 계산 구조를 변경함</li> </ul> <ol> <li>각 Key에 ReLU를 적용함: <strong>ReLU(K)</strong> </li> <li>두 가지 공유 term을 미리 계산 <ul> <li>\(\sum_{j=1}^N \mathrm{ReLU}(K_j)^\top V_j\) (d×d matrix)</li> <li>\(\sum_{j=1}^N \mathrm{ReLU}(K_j)^\top\) (d×1 vector)</li> </ul> </li> <li>이후, 각 Query에 대해 이 pre-computed shared term을 재사용하여 Attention을 계산 <ul> <li>이 방식은 Query마다 개별적으로 연산할 필요가 없어서, 전체 Attention 계산이 <strong>O(N)</strong> 으로 줄어듦.</li> </ul> </li> </ol> <p><strong>PixArt도 Linear이랬는데 다른점은?</strong></p> <p>PixArt에서는 Key와 Value 토큰을 압축하여 연산량을 줄여서 Engineering Optimization으로 O(N)과 비슷하게 하는 방식, SANA에서는 아예 수학적으로 계산량이 O(N)</p> <h3 id="mix-ffn-block">Mix-FFN Block</h3> <figure> <picture> <img src="/files/2025-05-12-sana/image%2010.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>기존 Transformer의 FFN (Feed-Forward Network)은 단순히 2개의 Linear Layer로 구성되어 있었음.</p> <p>FFN은 전역적인 정보는 잘 처리하지만, <strong>지역적인(local) 디테일</strong> 복원에는 약했음.</p> <p>SANA의 해결책:</p> <ul> <li>기존 MLP 사이에 <strong>3×3 Depthwise Convolution</strong>을 삽입</li> <li>이를 통해 지역 구조(local structure) 학습을 강화</li> <li>결과적으로 <strong>텍스처, 경계선</strong>, 이런 세밀한 부분 복원에 유리</li> </ul> <h3 id="dit-without-positional-encoding-nope">DiT without Positional Encoding (NoPE)</h3> <p>기존 Transformer 구조는 입력 순서를 인식하도록 <strong>Positional Encoding</strong>을 사용했음.</p> <p>왜냐면 Transformer는 입력 순서를 구별할 수 없었기 때문에…</p> <p>하지만 4K 고해상도 latent처럼 토큰 수가 많을 때, Positional Encoding을 계산하고 저장하는 데도 비용이 큼.</p> <p>SANA에서는 아예 Positional Encoding을 제거함</p> <ul> <li> <strong>3×3 Depthwise Convolution</strong>이 Mix-FFN에 추가되어서 지역적 위치 관계를 학습할 수 있음.</li> <li> <p>Linear Attention은 <strong>전역 관계</strong>를 자연스럽게 포착할 수 있음.</p> <p>→ 별도로 위치 정보를 부여하지 않아도 충분히 패턴과 구조를 학습할 수 있음.</p> </li> </ul> <p>결과적으로 품질이 유지되면서 구조가 간단해지고 메모리 연산량이 감소함</p> <figure> <picture> <img src="/files/2025-05-12-sana/image%2011.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="triton-acceleration-traininginference">Triton Acceleration Training/Inference</h3> <p>Appendix에 추가한다고 되어있는데 아직 관련 내용 없음.</p> <p>GPT가 알려준 내용</p> <ul> <li>Linear Attention을 구현할 때, 단순히 알고리즘만 개선하는 것으로는 부족함.</li> <li>실제 연산 효율까지 극대화하려면, <strong>GPU kernel 레벨 최적화</strong>가 필요함.</li> <li>SANA는 <strong>Triton</strong>을 사용하여 Linear Attention 연산을 직접 최적화함. <ul> <li>Triton은 NVIDIA가 지원하는 <strong>커스텀 GPU 커널 프로그래밍 프레임워크</strong>임.</li> <li>CUDA보다 단순한 문법으로, 고성능 커널을 작성할 수 있음.</li> </ul> </li> </ul> <p>Triton으로 최적화한 결과:</p> <ul> <li>Matrix 곱 연산(GEMM)과 Memory Access를 줄임.</li> <li>실제 latency(지연 시간)와 memory bandwidth 소모를 크게 개선</li> </ul> <h2 id="23-text-encoder-design">2.3 TEXT ENCODER DESIGN</h2> <h3 id="왜-t5-대신-decoder-only-llm을-사용하는가"><strong>왜 T5 대신 Decoder-only LLM을 사용하는가?</strong></h3> <p>SANA는 Gemma를 Text Encoder로 사용하기로 채택</p> <table> <thead> <tr> <th>항목</th> <th>기존 (T5)</th> <th>SANA (Gemma-2)</th> </tr> </thead> <tbody> <tr> <td>모델 구조</td> <td>Encoder-Decoder</td> <td>Decoder-only</td> </tr> <tr> <td>Reasoning 능력</td> <td>제한적</td> <td>매우 강함 (CoT, ICL 가능)</td> </tr> <tr> <td>추론 속도</td> <td>느림 (T5-XXL)</td> <td>6배 빠름 (Gemma-2-2B)</td> </tr> </tbody> </table> <figure> <picture> <img src="/files/2025-05-12-sana/image%2012.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ 빠른데도 불구하고, CLIP Score와 FID(이미지 품질 지표)에서는 <strong>성능이 비슷함</strong></p> <h3 id="decoder-only-llm을-text-encoder로-쓰면서-생긴-문제-해결">Decoder-only LLM을 Text Encoder로 쓰면서 생긴 문제 해결</h3> <p>Decoder-only LLM (Gemma, Qwen 등)의 텍스트 임베딩은 Variance가 훨씬 큼.</p> <ul> <li>큰 값이 텍스트 임베딩 안에 많이 포함되어 있음.</li> <li>Cross-Attention 연산 중 수치 폭발(NaN)로 이어짐.</li> </ul> <p><strong>방법 1: RMSNorm 추가</strong></p> <p>Gemma-2의 텍스트 임베딩 출력에 <strong>RMSNorm</strong>을 적용</p> <p>RMSNorm?</p> <ul> <li>입력 벡터의 Variance를 1.0으로 정규화</li> <li>큰 값이나 작은 값들을 균일하게 만들어 수치 폭발 방지</li> </ul> <p><strong>방법 2: Learnable Scale Factor 추가</strong></p> <ul> <li>추가로, 텍스트 임베딩에 <strong>학습 가능한 작은 스케일 파라미터</strong>를 곱함</li> <li>초기 값은 매우 작게 설정함 (예: 0.01)</li> <li>이 파라미터가 학습을 통해 적절한 크기로 조정되면서 모델 수렴 속도가 빨라짐</li> </ul> <p>→ 훈련 안정성 확보 + 수렴 속도 향상</p> <figure> <picture> <img src="/files/2025-05-12-sana/image%2013.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="complex-human-instruction-improves-text-image-alignment">Complex Human Instruction Improves Text-Image Alignment</h3> <p>Gemma는 강력한 LLM이지만, 사용자가 짧거나 모호한 프롬프트를 입력하면 (예: “a cat”)</p> <p>LLM이 초점을 잃고 엉뚱한 답변을 할 수도 있음.</p> <p>→ LLM이 프롬프트에만 집중하게 만드는 추가 지시문이 필요함.</p> <p><strong>CHI가 그래서 뭔디?</strong></p> <p>LLM의 <strong>In-Context Learning</strong> 능력을 활용하여 프롬프트를 주기 전에,</p> <p>LLM에게 “색상, 크기, 위치 관계 같은 세부 묘사를 추가해라”와 같은 <strong>복잡한 명령 세트</strong>를 함께 제공하는 것</p> <p><strong>결과 1</strong></p> <figure> <picture> <img src="/files/2025-05-12-sana/image%2014.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>CHI를 적용했을 때, 학습을 처음부터 하든(fresh training)</p> <p>아니면 기존 모델을 미세 조정(fine-tuning)하든</p> <p><strong>텍스트-이미지 정렬 성능이 향상</strong></p> <p><strong>결과 2</strong></p> <figure> <picture> <img src="/files/2025-05-12-sana/image%2015.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>짧은 프롬프트(예: “a cat”)를 입력했을 때,</p> <p>CHI가 없으면, 모델이 엉뚱한 이미지를 생성하거나 품질이 불안정해짐.</p> <p>CHI가 있으면, 모델이 <strong>프롬프트에 정확히 맞는 안정적인 이미지</strong>를 생성</p> <h1 id="3-efficient-traininginference">3 EFFICIENT TRAINING/INFERENCE</h1> <h2 id="31-data-curation-and-blending">3.1 DATA CURATION AND BLENDING</h2> <h3 id="1-multi-caption-auto-labelling-pipeline">1. Multi-Caption Auto-labelling Pipeline</h3> <p><strong>이미지 하나당 4개의 VLM(Vision-Language Models) 을 이용해 캡션을 생성함.</strong></p> <ul> <li><strong>VILA-3B</strong></li> <li><strong>VILA-13B</strong></li> <li><strong>InternVL-28B</strong></li> <li><strong>InternVL-26B</strong></li> </ul> <p><strong>→ 정확한 캡션</strong> 생성 (하나만 쓰는 것보다 오류 줄임)</p> <p><strong>→ 다양한 표현</strong> 확보 (같은 이미지를 여러 관점에서 묘사 가능)</p> <hr> <h3 id="2-clip-score-based-caption-sampler">2. CLIP-Score-based Caption Sampler</h3> <p><strong>문제 상황</strong></p> <ul> <li> <p>캡션을 여러 개 만들었는데,</p> <p>훈련할 때 어떤 캡션을 선택할지가 문제임.</p> </li> <li> <p>무작위로(random) 하나 고르면:</p> <ul> <li>품질이 낮은 문장을 뽑을 위험이 있음</li> <li>그러면 훈련이 느려지거나 모델 품질이 떨어짐</li> </ul> </li> </ul> <p><strong>해결 방법</strong></p> <ul> <li> <strong>CLIP score</strong>를 활용해 품질 높은 캡션을 뽑는 방식 사용 <ul> <li>CLIP은 이미지-텍스트 매칭 정도를 점수로 계산해줌.</li> </ul> </li> <li>과정: <ol> <li>이미지에 대해 생성된 각 캡션의 CLIP 점수(cic_ici)를 계산</li> <li>점수가 높은 캡션일수록 뽑힐 확률이 높게 설정</li> <li>Sampling 확률 공식:</li> </ol> </li> </ul> \[P(c_i) = \frac{e^{c_i / \tau}}{\sum_{j=1}^N e^{c_j / \tau}}\] <p>여기</p> <ul> <li>τ는 “temperature”라는 하이퍼파라미터임.</li> <li> <strong>Temperature 조정으로 뽑는 강도를 조절</strong>할 수 있음: <ul> <li>τ가 작으면: 점수 가장 높은 캡션만 거의 항상 선택</li> <li>τ가 크면: 다양한 캡션이 고르게 선택됨</li> </ul> </li> </ul> <hr> <h3 id="실험-결과">실험 결과</h3> <ul> <li>Table 4 결과에 따르면: <ul> <li>캡션을 다양하게 골라도 이미지 품질(FID)은 거의 변하지 않음</li> <li>하지만 <strong>훈련 중 텍스트-이미지 정렬</strong>(semantic alignment)은 훨씬 좋아짐</li> </ul> </li> </ul> <hr> <h3 id="3-cascade-resolution-training"><strong>3. Cascade Resolution Training</strong></h3> <h3 id="기존-방식">기존 방식</h3> <ul> <li>대부분의 diffusion 모델은 해상도 256px짜리 이미지로 먼저 pre-training을 함.</li> <li>이유는 연산 비용(cost)을 줄이기 위해서임.</li> </ul> <h3 id="문제점">문제점</h3> <ul> <li>256px 이미지는 디테일(detail) 손실이 심함.</li> <li>따라서, 작은 해상도에서 학습을 시작하면: <ul> <li>모델이 fine한 구조나 텍스처를 배우기 어려움</li> <li>결국 큰 해상도로 갈 때 더 느리게 학습함.</li> </ul> </li> </ul> <hr> <h3 id="sana-방식">SANA 방식</h3> <ul> <li> <p>SANA는 <strong>AE-F32C32P1 구조</strong>를 사용하기 때문에</p> <p>latent 공간이 매우 작음 → 연산 부담이 적음.</p> </li> <li>그래서 굳이 256px에서 시작할 필요 없이 바로 <strong>512px</strong>에서 학습 시작함.</li> <li>학습 순서: <ul> <li>512px → 1024px → 2K → 4K 순서로 점진적(fine-tuning)으로 해상도를 올림.</li> </ul> </li> </ul> <h2 id="32-flow-based-training--inference">3.2 FLOW-BASED TRAINING / INFERENCE</h2> <h3 id="flow-based-training">Flow-based Training</h3> <p>기존 방식 : noise prediction</p> \[x_t = \alpha_t x_0 + \sigma_t \epsilon\] \[\epsilon_\theta(x_t, t) = \epsilon\] <ul> <li>\(x_0\): 원본 이미지</li> <li>\(\epsilon\): 랜덤 노이즈</li> <li>\(\alpha_t, \sigma_t\): diffusion 과정의 하이퍼파라미터</li> </ul> <p>→ 노이즈를 맞추는 것이 학습 목표</p> <p>문제점 : t가 커지면 (Diffusion 마지막 단계에 가까우면) 노이즈가 커져서 예측 불안정</p> <p>새 방식 : EDM ,RF</p> <p>noise 대신 data나 velocity(노이즈와 원본 이미지 차이) 예측</p> <table> <thead> <tr> <th>방법</th> <th>목표</th> </tr> </thead> <tbody> <tr> <td>EDM</td> <td>\(x_\theta(x_t, t) = x_0\) (원본 데이터 예측)</td> </tr> <tr> <td>RF</td> <td>\(v_\theta(x_t, t) = \epsilon - x_0\) (velocity 예측)</td> </tr> </tbody> </table> \[v_\theta(x_t, t) = \epsilon - x_0\] \[x_0 = x_T - \sigma_T \cdot v_\theta(x_T, t_T)\] <p>결국 RF를 사용하여 cumulative(누적) error를 줄일 수 있음</p> <h3 id="flow-based-inference">Flow-based Inference</h3> <p>기존 : DPM-Solver++</p> <p>→ required 28-50 steps for high-quality samples</p> <p>현재 : <strong>Flow-DPM-Solver</strong></p> <p>1.Not predict original data, but velocity</p> <p>2.substituting the scaling factor αt with 1 − σt</p> <p>3.time-steps are redefined over the range [0, 1] instead of [1, 1000]</p> <p>→Generate high-quality samples in 14-20 steps</p> <h1 id="5-experiments">5. Experiments</h1> <h2 id="1-model-details">1. Model Details</h2> <h3 id="sana-06b">Sana-0.6B</h3> <ul> <li>파라미터 수: <strong>590M</strong> </li> <li>구조: <strong>DiT-XL</strong> 및 <strong>PixArt-Σ</strong>와 거의 동일한 레이어 수와 채널 수 사용</li> <li>목적: 소형 모델로도 효율성과 품질을 동시에 확보</li> </ul> <h3 id="sana-16b">Sana-1.6B</h3> <ul> <li>파라미터 수: <strong>1.6B</strong> </li> <li>구조: <ul> <li><strong>20개의 Transformer 레이어</strong></li> <li>각 레이어마다 <strong>2240개의 채널</strong> </li> <li>FFN 내부 채널 수는 <strong>5600</strong> </li> </ul> </li> <li>이 구성은 학습 효율성과 생성 품질 사이의 균형을 고려한 것임</li> </ul> <h2 id="2-evaluation-details">2. Evaluation Details</h2> <p>SANA는 총 <strong>5가지 대표적인 평가 지표</strong>를 사용하여 성능을 평가함.</p> <table> <thead> <tr> <th>지표</th> <th>설명</th> </tr> </thead> <tbody> <tr> <td><strong>FID (Fréchet Inception Distance)</strong></td> <td>이미지 품질을 수치로 측정. 낮을수록 좋음</td> </tr> <tr> <td><strong>CLIP Score</strong></td> <td>이미지와 텍스트 간 의미적 정렬 정도 평가. 높을수록 좋음</td> </tr> <tr> <td> <strong>GenEval</strong> (Ghosh et al., 2024)</td> <td>텍스트-이미지 정렬 평가. 총 533개의 프롬프트 사용</td> </tr> <tr> <td> <strong>DPG-Bench</strong> (Hu et al., 2024)</td> <td>텍스트-이미지 정렬 정밀도 테스트. 1065개의 프롬프트 사용</td> </tr> <tr> <td> <strong>ImageReward</strong> (Xu et al., 2024)</td> <td>인간의 주관적 선호도를 반영한 점수. 100개 프롬프트로 측정</td> </tr> </tbody> </table> <h2 id="3-평가-데이터셋">3. 평가 데이터셋</h2> <ul> <li> <strong>MJHQ-30K (Li et al., 2024a)</strong> <ul> <li>Midjourney에서 수집한 <strong>30,000개 고품질 이미지</strong> 포함</li> <li>FID, CLIP Score 측정에 사용됨</li> </ul> </li> </ul> <figure> <picture> <img src="/files/2025-05-12-sana/image%2023.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/files/2025-05-12-sana/image%2024.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="limitation">Limitation</h1> <figure> <picture> <img src="/files/2025-05-12-sana/image%2025.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><a href="https://x.com/cloneofsimo/status/1864309440356470894?s=46" rel="external nofollow noopener" target="_blank">https://x.com/cloneofsimo/status/1864309440356470894?s=46</a></p> <p>코드가 전반적으로 NVIDIA칩만을 위해 설계되어</p> <p>다른 GPU 장비는 물론이고,</p> <p>Mobile Device에서는 당연히 불가능함.</p> <p>아무래도 NVIDIA에서 낸 논문이기 때문에 Blackwell chip 홍보 겸 NVIDIA chip에서만 가능하도록 한듯.</p> </div> </article> <br><br><br><br> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'hoonably/hoonably.github.io',
        'data-repo-id': 'R_kgDOPBaE1Q',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDOPBaE1c4Cr9sR',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '0',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'ko',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> <br><br><br><br> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeonghoon Park. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>